[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/data/geospatial/hexagon.html",
    "href": "In-class_Ex/In-class_Ex1/data/geospatial/hexagon.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "",
    "text": "In this in-class exercise, you are required to prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 downloaded from data.gov.sg.\n\nThe specific task of this in-class exercise are as follows:\n\nto import Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall in to RStudio environment,\nto import geospatial data in ESRI shapefile format into sf data frame format,\nto perform data wrangling by using appropriate functions from tidyverse and sf pakcges, and\nto visualise the distribution of passenger trip by using tmap methods and functions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-task",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-task",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "",
    "text": "In this in-class exercise, you are required to prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 downloaded from data.gov.sg.\n\nThe specific task of this in-class exercise are as follows:\n\nto import Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall in to RStudio environment,\nto import geospatial data in ESRI shapefile format into sf data frame format,\nto perform data wrangling by using appropriate functions from tidyverse and sf pakcges, and\nto visualise the distribution of passenger trip by using tmap methods and functions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Getting Started",
    "text": "Getting Started\nThree R packages will be used in this in-class exercise, they are:\n\ntidyverse for non-spatial data handling,\nsf for geospatial data handling,\ntmap for thematic mapping, and\nknitr for creating html table.\n\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, load these three R packages into RStudio.\n\n\n\npacman::p_load(tmap, sf, tidyverse, \n               knitr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Importing the OD data",
    "text": "Importing the OD data\nFirstly, we will import the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import origin_destination_bus_202308.csv downloaded from LTA DataMall into RStudio and save it as a tibble data frame called odbus.\n\n\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\n\n\n\nA quick check of odbus tibble data frame shows that the values in OROGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type.\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"4406…\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"1722…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\nThe taskThe solution\n\n\nUsing appropriate tidyverse functions to convert these data values into factor data type.\n\n\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\n\nNotice that both of them are in factor data type now.\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 44069, 20281, 2…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 17229, 20141, 2…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\nExtracting the study data\n\nThe taskThe solution\n\n\nFor the purpose of this exercise, we will extract commuting flows during the weekday morning peak. Call the output tibble data table as origin7_9.\n\n\n\norigin7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\n\n\nIt should look similar to the data table below.\n\nkable(head(origin7_9))\n\n\n\n\nORIGIN_PT_CODE\nTRIPS\n\n\n\n\n01012\n1617\n\n\n01013\n813\n\n\n01019\n1620\n\n\n01029\n2383\n\n\n01039\n2727\n\n\n01059\n1415\n\n\n\n\n\nWe will save the output in rds format for future used.\n\nwrite_rds(origin7_9, \"data/rds/origin7_9.rds\")\n\nThe code chunk below will be used to import the save origin7_9.rds into R environment.\n\norigin7_9 &lt;- read_rds(\"data/rds/origin7_9.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#working-with-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#working-with-geospatial-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Working with Geospatial Data",
    "text": "Working with Geospatial Data\nIn this section, you are required to import two shapefile into RStudio, they are:\n\nBusStop: This data provides the location of bus stop as at last quarter of 2022.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\n\nImporting geospatial data\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import BusStop downloaded from LTA DataMall into RStudio and save it as a sf data frame called busstop.\n\n\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n\nThe structure of busstop sf tibble data frame should look as below.\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import MPSZ-2019 downloaded from eLearn into RStudio and save it as a sf data frame called mpsz.\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\nThe structure of mpsz sf tibble data frame should look as below.\n\nglimpse(mpsz)\n\nRows: 332\nColumns: 7\n$ SUBZONE_N  &lt;chr&gt; \"MARINA EAST\", \"INSTITUTION HILL\", \"ROBERTSON QUAY\", \"JURON…\n$ SUBZONE_C  &lt;chr&gt; \"MESZ01\", \"RVSZ05\", \"SRSZ01\", \"WISZ01\", \"MUSZ02\", \"MPSZ05\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA EAST\", \"RIVER VALLEY\", \"SINGAPORE RIVER\", \"WESTERN …\n$ PLN_AREA_C &lt;chr&gt; \"ME\", \"RV\", \"SR\", \"WI\", \"MU\", \"MP\", \"WI\", \"WI\", \"SI\", \"SI\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"WEST…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"WR\", \"CR\", \"CR\", \"WR\", \"WR\", \"CR\", \"CR\",…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((33222.98 29..., MULTIPOLYGON (…\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() function of sf package is used to import the shapefile into R as sf data frame.\nst_transform() function of sf package is used to transform the projection to crs 3414."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Geospatial data wrangling",
    "text": "Geospatial data wrangling\n\nCombining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame.\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore bpundary.\n\n\n\nBefore moving to the next step, it is wise to save the output into rds format.\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.csv\")  \n\n\nThe taskThe solution\n\n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus7_9 data frame.\n\n\n\norigin_SZ &lt;- left_join(origin7_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C) %&gt;%\n  group_by(ORIGIN_SZ) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\n\n\n\nBefore continue, it is a good practice for us to check for duplicating records.\n\nduplicate &lt;- origin_SZ %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nIf duplicated records are found, the code chunk below will be used to retain the unique records.\n\norigin_data &lt;- unique(origin_SZ)\n\nIt will be a good practice to confirm if the duplicating records issue has been addressed fully.\n\nThe taskThe solution\n\n\nNext, write a code chunk to update od_data data frame with the planning subzone codes.\n\n\n\norigintrip_SZ &lt;- left_join(mpsz, \n                           origin_SZ,\n                           by = c(\"SUBZONE_C\" = \"ORIGIN_SZ\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#choropleth-visualisation",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#choropleth-visualisation",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Choropleth Visualisation",
    "text": "Choropleth Visualisation\n\nThe taskThe solution\n\n\nUsing the steps you had learned, prepare a choropleth map showing the distribution of passenger trips at planning sub-zone level.\n\n\n\ntm_shape(origintrip_SZ)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips generated at planning sub-zone level\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from URA\\n and Passenger trips data from LTA\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nCreating interactive map\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(origintrip_SZ)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips generated at planning sub-zone level\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from URA\\n and Passenger trips data from LTA\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "",
    "text": "Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#installing-and-loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#installing-and-loading-the-r-packages",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Installing and Loading the R Packages",
    "text": "Installing and Loading the R Packages\nAs usual, p_load() of pacman package will be used to check if the necessary packages have been installed in R, if yes, load the packages on R environment.\nFive R packages are need for this in-class exercise, they are: sf, sfdep, tmap, plotly and tidyverse.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\n\n\n\nShow the code\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#importing-geospatial-data",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\nIn the code chunk below, st_read() of sf package is used to import Hunan shapefile into R.\n\n\n\nShow the code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nDo it Yourself\nUsing the steps you learned in previous lesson, examine the content hunan sf data.frame"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#importing-attribute-table",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#importing-attribute-table",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Importing attribute table",
    "text": "Importing attribute table\nIn the code chunk below, read_csv() of readr is used to import Hunan_GDPPC.csv into R.\n\n\n\nShow the code\nGDPPC &lt;- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\n\n\n\n\nDo it Yourself\nUsing the steps you learned in previous lesson, examine the content the GDPPC tibble data.frame."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#computing-gi",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#computing-gi",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nNext, we will compute the local Gi* statistics.\n\nDeriving the spatial weights\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weights.\n\n\n\nShow the code\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nactivate() of dplyr package is used to activate the geometry context\nmutate() of dplyr package is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\n\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\nhead(GDPPC_nb)\n\n# A tibble: 6 × 5\n   Year County  GDPPC nb        wt       \n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;   \n1  2005 Anxiang  8184 &lt;int [6]&gt; &lt;dbl [6]&gt;\n2  2005 Hanshou  6560 &lt;int [6]&gt; &lt;dbl [6]&gt;\n3  2005 Jinshi   9956 &lt;int [5]&gt; &lt;dbl [5]&gt;\n4  2005 Li       8394 &lt;int [5]&gt; &lt;dbl [5]&gt;\n5  2005 Linli    8850 &lt;int [5]&gt; &lt;dbl [5]&gt;\n6  2005 Shimen   9244 &lt;int [6]&gt; &lt;dbl [6]&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#computing-gi-1",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#computing-gi-1",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\n\n\nShow the code\ngi_stars &lt;- GDPPC_nb %&gt;% \n  group_by(Year) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#arrange-to-show-significant-emerging-hotcold-spots",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#arrange-to-show-significant-emerging-hotcold-spots",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Arrange to show significant emerging hot/cold spots",
    "text": "Arrange to show significant emerging hot/cold spots\n\n\n\nShow the code\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#performing-emerging-hotspot-analysis",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-EHSA.html#performing-emerging-hotspot-analysis",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Performing Emerging Hotspot Analysis",
    "text": "Performing Emerging Hotspot Analysis\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n\n\n\nShow the code\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st, \n  .var = \"GDPPC\", \n  k = 1, \n  nsim = 99\n)\n\n\n\n\nVisualising the distribution of EHSA classes\nIn the code chunk below, ggplot2 functions ised used to reveal the distribution of EHSA classes as a bar chart.\n\n\n\nShow the code\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\n\nFigure above shows that sporadic cold spots class has the high numbers of county.\n\n\nVisualising EHSA\nIn this section, you will learn how to visualise the geographic distribution EHSA classes. However, before we can do so, we need to join both hunan and ehsa together by using the code chunk below.\n\n\n\nShow the code\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\n\n\nNext, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.\n\n\n\nShow the code\nehsa_sig &lt;- hunan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Chapter 9: Global Measures of Spatial Autocorrelation and Chapter 10: Local Measures of Spatial Autocorrelation. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#overview",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#overview",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Chapter 9: Global Measures of Spatial Autocorrelation and Chapter 10: Local Measures of Spatial Autocorrelation. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#getting-started",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#getting-started",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\n\n\n\nShow the code\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#the-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#the-data",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nImporting geospatial data\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\n\nShow the code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nImporting attribute table\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\n\nShow the code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nCombining both data frame by using left join\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)\n\n\n\n\n\nShow the code\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nPlotting a choropleth map\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\nThe choropleth should look similar to the figure below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#global-measures-of-spatial-association",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#global-measures-of-spatial-association",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Global Measures of Spatial Association",
    "text": "Global Measures of Spatial Association\n\nStep 1: Deriving contiguity weights: Queen’s method\n\n\n\n\n\n\nDo it Yourself!\n\n\n\nUsing the steps you learned in previous lesson, derive a Queen’s contiguity weights by using appropriate spdep and tidyverse functions.\n\n\n\n\nDeriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\nComputing Global Moran’ I\nIn the code chunk below, global_moran() function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.\n\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\n\nPerforming Global Moran’sI test\nIn general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using global_moran_test() as shown in the code chunk below.\n\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe default for alternative argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality.\n\n\n\n\n\nPerforming Global Moran’I permutation test\nIn practice, monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_moran_perm()\nIt is always a good practice to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\nset.seed(1234)\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nThe report above show that the p-value is smaller than alpha value of 0.05. Hence, reject the null hypothesis that the spatial patterns spatial independent. Because the Moran’s I statistics is greater than 0. We can infer the spatial distribution shows sign of clustering.\n\n\n\n\n\n\nReminder\n\n\n\nThe numbers of simulation is alway equal to nsim + 1. This mean in nsim = 99. This mean 100 simulation will be performed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#computing-local-morans-i",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\nIn this section, you will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations\np_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations\np_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative=\np_folded_sim: the simulation folded [0, 0.5] range ranked p-value based on crand.py of pysal\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.\n\n\n\n\n\n\n\nImportant\n\n\n\nunnest() of tidyr package is used to expand a list-column containing data frames into rows and columns.\n\n\n\nVisualising local Moran’s I\nIn this code chunk below, tmap functions are used prepare a choropleth map by using value in the ii field.\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\nVisualising p-value of local Moran’s I\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor p-values, the appropriate classification should be 0.001, 0.01, 0.05 and not significant instead of using default classification scheme.\n\n\n\n\nVisualising local Moran’s I and p-value\nFor effective comparison, it will be better for us to plot both maps next to each other as shown below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\nVisualising LISA map\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "Hot Spot and Cold Spot Area Analysis (HCSA)\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#computing-local-gi-statistics",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#computing-local-gi-statistics",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Computing local Gi* statistics",
    "text": "Computing local Gi* statistics\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, derive an inverse distance weights matrix.\n\n\n\n\n\nShow the code\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\nNext, local_gstar_perm() of sfdep package will be used to compute local Gi* statistics as shown in the code chunk below.\n\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 499),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 16 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 17\n   gi_star   e_gi    var_gi p_value   p_sim p_folded_sim skewness kurtosis nb   \n     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;nb&gt; \n 1  0.0416 0.0114   6.24e-6  0.0472 9.62e-1        0.84     0.42     0.739 &lt;int&gt;\n 2 -0.333  0.0112   6.39e-6 -0.301  7.64e-1        0.932    0.466    0.852 &lt;int&gt;\n 3  0.281  0.0125   7.83e-6 -0.0911 9.27e-1        0.872    0.436    1.01  &lt;int&gt;\n 4  0.411  0.0113   7.14e-6  0.508  6.11e-1        0.568    0.284    0.868 &lt;int&gt;\n 5  0.387  0.0114   7.81e-6  0.421  6.74e-1        0.54     0.27     1.25  &lt;int&gt;\n 6 -0.368  0.0116   6.81e-6 -0.478  6.33e-1        0.764    0.382    0.914 &lt;int&gt;\n 7  3.56   0.0146   7.04e-6  2.84   4.56e-3        0.032    0.016    1.09  &lt;int&gt;\n 8  2.52   0.0135   5.08e-6  1.69   9.14e-2        0.148    0.074    0.797 &lt;int&gt;\n 9  4.56   0.0141   4.57e-6  4.12   3.71e-5        0.008    0.004    1.03  &lt;int&gt;\n10  1.16   0.0109   4.92e-6  1.35   1.76e-1        0.208    0.104    0.597 &lt;int&gt;\n# ℹ 78 more rows\n# ℹ 8 more variables: wts &lt;list&gt;, NAME_2 &lt;chr&gt;, ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;,\n#   ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;, geometry &lt;POLYGON [°]&gt;\n\n\n\n\nVisualising Gi*\n\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\nVisualising p-value of HCSA\n\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nVisuaising local HCSA\nFor effective comparison, you can plot both maps next to each other as shown below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#visualising-hot-spot-and-cold-spot-areas",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#visualising-hot-spot-and-cold-spot-areas",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Visualising hot spot and cold spot areas",
    "text": "Visualising hot spot and cold spot areas\nNow, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\nFigure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#overview",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#overview",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#getting-started",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#getting-started",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap, tidyverse.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep, tidyverse and knitr packages into R environment.\n\n\n\n\nShow the code\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#the-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#the-data",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nImporting geospatial data\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\nShow the code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImporting attribute table\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\nShow the code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\nCombining both data frame by using left join\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\nShow the code\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)\n\n\n\n\nPlotting a choropleth map\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\nThe choropleth should look similar to ther figure below.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#deriving-contiguity-spatial-weights",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#deriving-contiguity-spatial-weights",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "Deriving Contiguity Spatial Weights",
    "text": "Deriving Contiguity Spatial Weights\nBy and large, there are two types of spatial weights, they are contiguity wights and distance-based weights. In this section, you will learn how to derive contiguity spatial weights by using sfdep.\nTwo steps are required to derive a contiguity spatial weights, they are:\n\nidentifying contiguity neighbour list by st_contiguity() of sfdep package, and\nderiving the contiguity spatial weights by using st_weights() of sfdep package\n\nIn this section, we will learn how to derive the contiguity neighbour list and contiguity spatial weights separately. Then, we will learn how to combine both steps into a single process.\n\nIdentifying contiguity neighbours: Queen’s method\nIn the code chunk below st_contiguity() is used to derive a contiguity neighbour list by using Queen’s method.\n\nnb_queen &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, queen argument is TRUE. If you do not specify queen = FALSE, this function will return a list of first order neighbours by using the Queen criteria. Rooks method will be used to identify the first order neighbour if queen = FALSE is used.\n\n\nThe code chunk below is used to print the summary of the first lag neighbour list (i.e. nb) .\n\nsummary(nb_queen$nb)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan province. The most connected area unit has 11 neighbours. There are two are units with only one neighbour.\nTo view the content of the data table, you can either display the output data frame on RStudio data viewer or by printing out the first ten records by using the code chunk below.\n\nnb_queen\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb   NAME_2  ID_3    NAME_3   ENGTYPE_3\n1                 2, 3, 4, 57, 85  Changde 21098   Anxiang      County\n2               1, 57, 58, 78, 85  Changde 21100   Hanshou      County\n3                     1, 4, 5, 85  Changde 21101    Jinshi County City\n4                      1, 3, 5, 6  Changde 21102        Li      County\n5                     3, 4, 6, 85  Changde 21103     Linli      County\n6                4, 5, 69, 75, 85  Changde 21104    Shimen      County\n7                  67, 71, 74, 84 Changsha 21109   Liuyang County City\n8       9, 46, 47, 56, 78, 80, 86 Changsha 21110 Ningxiang      County\n9           8, 66, 68, 78, 84, 86 Changsha 21111 Wangcheng      County\n10 16, 17, 19, 20, 22, 70, 72, 73 Chenzhou 21112     Anren      County\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\nThe print shows that polygon 1 has five neighbours. They are polygons number 2, 3, 4, 57,and 85.\nOne of the advantage of sfdep over spdep is that the output is an sf tibble data frame.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, display nb_queen sf tibble data frame in a table display.\n\n\n\n\nShow the code\nkable(head(nb_queen,\n           n=10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnb\nNAME_2\nID_3\nNAME_3\nENGTYPE_3\nCounty\nGDPPC\ngeometry\n\n\n\n\n2, 3, 4, 57, 85\nChangde\n21098\nAnxiang\nCounty\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\n1, 57, 58, 78, 85\nChangde\n21100\nHanshou\nCounty\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\n1, 4, 5, 85\nChangde\n21101\nJinshi\nCounty City\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…\n\n\n1, 3, 5, 6\nChangde\n21102\nLi\nCounty\nLi\n24473\nPOLYGON ((111.3731 29.94649…\n\n\n3, 4, 6, 85\nChangde\n21103\nLinli\nCounty\nLinli\n25554\nPOLYGON ((111.6324 29.76288…\n\n\n4, 5, 69, 75, 85\nChangde\n21104\nShimen\nCounty\nShimen\n27137\nPOLYGON ((110.8825 30.11675…\n\n\n67, 71, 74, 84\nChangsha\n21109\nLiuyang\nCounty City\nLiuyang\n63118\nPOLYGON ((113.9905 28.5682,…\n\n\n9, 46, 47, 56, 78, 80, 86\nChangsha\n21110\nNingxiang\nCounty\nNingxiang\n62202\nPOLYGON ((112.7181 28.38299…\n\n\n8, 66, 68, 78, 84, 86\nChangsha\n21111\nWangcheng\nCounty\nWangcheng\n70666\nPOLYGON ((112.7914 28.52688…\n\n\n16, 17, 19, 20, 22, 70, 72, 73\nChenzhou\n21112\nAnren\nCounty\nAnren\n12761\nPOLYGON ((113.1757 26.82734…\n\n\n\n\n\n\n\nIdentify contiguity neighbours: Rooks’ method\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity neighbour list using Rooks’ method.\n\n\n\n\nShow the code\nnb_rook &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         .before = 1)\n\n\n\n\nIdentifying higher order neighbors\nThere are times that we need to identify high order contiguity neighbours. To accomplish the task, st_nb_lag_cumul() should be used as shown in the code chunk below.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity neighbour list using lag 2 Queen’s method.\n\n\n\n\nShow the code\nnb2_queen &lt;-  hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\n\nNote that if the order is 2, the result contains both 1st and 2nd order neighbors as shown on the print below.\n\nnb2_queen\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                        nb2\n1                                     2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\n2                           1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\n3                                                 1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\n4                                                             1, 2, 3, 5, 6, 57, 69, 75, 85\n5                                                 1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\n6                                         1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\n7                                                     9, 19, 66, 67, 71, 73, 74, 76, 84, 86\n8  2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\n9               2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\n10               11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#deriving-contiguity-weights-queens-method",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#deriving-contiguity-weights-queens-method",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "Deriving contiguity weights: Queen’s method",
    "text": "Deriving contiguity weights: Queen’s method\nNow, you are ready to compute the contiguity weights by using st_weights() of sfdep package.\n\nDeriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#distance-based-weights",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#distance-based-weights",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "Distance-based Weights",
    "text": "Distance-based Weights\nThere are three popularly used distance-based spatial weights, they are:\n\nfixed distance weights,\nadaptive distance weights, and\ninverse distance weights (IDW).\n\n\nDeriving fixed distance weights\nBefore we can derive the fixed distance weights, we need to determine the upper limit for distance band by using the steps below:\n\ngeo &lt;- sf::st_geometry(hunan_GDPPC)\nnb &lt;- st_knn(geo, longlat = TRUE)\ndists &lt;- unlist(st_nb_dists(geo, nb))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_nb_dists() of sfdep is used to calculate the nearest neighbour distance. The output is a list of distances for each observation’s neighbors list.\nunlist() of Base R is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived.\n\n\n\nNow, we will go ahead to derive summary statistics of the nearest neighbour distances vector (i.e. dists) by usign the coced chunk below.\n\nsummary(dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nThe summary statistics report above shows that the maximum nearest neighbour distance is 65.80km. By using a threshold value of 66km will ensure that each area will have at least one neighbour.\nNow we will go ahead to compute the fixed distance weights by using the code chunk below.\n\nwm_fd &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_dist_band(geometry,\n                           upper = 66),\n               wt = st_weights(nb),\n               .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_dists_band() of sfdep is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\n\n\n\n\nDo It Yourself\n\n\n\nUsing the steps you learned in previous section, examine the data frame of the fixed distance weights.\n\n\n\n\nDeriving adaptive distance weights\nIn this section, you will derive an adaptive spatial weights by using the code chunk below.\n\nwm_ad &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_knn(geometry,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_knn() of sfdep is used to identify neighbors based on k (i.e. k = 8 indicates the nearest eight neighbours). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\nCalculate inverse distance weights\nIn this section, you will derive an inverse distance weights by using the code chunk below.\n\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_contiguity() of sfdep is used to identify the neighbours by using contiguity criteria. The output is a list of neighbours (i.e. nb).\nst_inverse_distance() is then used to calculate inverse distance weights of neighbours on the nb list."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/data/geospatial/Business.html",
    "href": "In-class_Ex/In-class_Ex4/data/geospatial/Business.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/data/geospatial/entertn.html",
    "href": "In-class_Ex/In-class_Ex4/data/geospatial/entertn.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/data/geospatial/F&B.html",
    "href": "In-class_Ex/In-class_Ex4/data/geospatial/F&B.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/data/geospatial/FinServ.html",
    "href": "In-class_Ex/In-class_Ex4/data/geospatial/FinServ.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/data/geospatial/Liesure&Recreation.html",
    "href": "In-class_Ex/In-class_Ex4/data/geospatial/Liesure&Recreation.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex4/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/data/geospatial/Retails.html",
    "href": "In-class_Ex/In-class_Ex4/data/geospatial/Retails.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html",
    "title": "In-class Exercise 4: Preparing Spatial Interaction Modelling Variables",
    "section": "",
    "text": "A healthy baby need healthy food. Likewise, a well calibrated Spatial Interaction Model need conceptually logical and well prepared propulsiveness and attractiveness variables. In this in-class exercise, you will gain hands-on experience on preparing propulsiveness and attractiveness variables require for calibrating spatial interaction models. By the end of this in-class exercise, you will be able to:\n\nperform geocoding by using SLA OneMap API,\nconvert an aspatial data into a simple feature tibble data.frame,\nperform point-in-polygon count analysis, and\nappend the propulsiveness and attractiveness variables onto a flow data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html#overview",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html#overview",
    "title": "In-class Exercise 4: Preparing Spatial Interaction Modelling Variables",
    "section": "",
    "text": "A healthy baby need healthy food. Likewise, a well calibrated Spatial Interaction Model need conceptually logical and well prepared propulsiveness and attractiveness variables. In this in-class exercise, you will gain hands-on experience on preparing propulsiveness and attractiveness variables require for calibrating spatial interaction models. By the end of this in-class exercise, you will be able to:\n\nperform geocoding by using SLA OneMap API,\nconvert an aspatial data into a simple feature tibble data.frame,\nperform point-in-polygon count analysis, and\nappend the propulsiveness and attractiveness variables onto a flow data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html#getting-started",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html#getting-started",
    "title": "In-class Exercise 4: Preparing Spatial Interaction Modelling Variables",
    "section": "Getting Started",
    "text": "Getting Started\nTo get start, the following R packages will be loaded into R environment. They are:\n\ntidyverse, provide a family of modern R packages for data import, wrangling\n\n\npacman::p_load(tidyverse, sf, httr,\n               tmap)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html#counting-number-of-schools-in-each-ura-planning-subzone",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html#counting-number-of-schools-in-each-ura-planning-subzone",
    "title": "In-class Exercise 4: Preparing Spatial Interaction Modelling Variables",
    "section": "Counting number of schools in each URA Planning Subzone",
    "text": "Counting number of schools in each URA Planning Subzone\n\nDownloading General information of schools data from data.gov.sg\nTo get started, you are required to download General information of schools data set of School Directory and Information from data.gov.sg.\n\n\n\n\n\n\nImportant\n\n\n\nWe assume that the downloaded School Directory and Information is placed in a sub-folder called data/aspatial/).\n\n\n\n\nGeocoding using SLA API\nAddress geocoding, or simply geocoding, is the process of taking a aspatial description of a location, such as an address or postcode, and returning geographic coordinates, frequently latitude/longitude pair, to identify a location on the Earth’s surface.\nSingapore Land Authority (SLA) supports an online geocoding service called OneMap API. The Search API looks up the address data or 6-digit postal code for an entered value. It then returns both latitude, longitude and x,y coordinates of the searched location.\nThe code chunks below will perform geocoding using SLA OneMap API. The input data will be in csv file format. It will be read into R Studio environment using read_csv function of readr package. A collection of http call functions of httr package of R will then be used to pass the individual records to the geocoding server at OneMap.\nTwo tibble data.frames will be created if the geocoding process completed successfully. They are called found and not_found. found contains all records that are geocoded correctly and not_found contains postal that failed to be geocoded.\nLastly, the found data table will joined with the initial csv data table by using a unique identifier (i.e. POSTAL) common to both data tables. The output data table will then save as an csv file called found.\n\nurl&lt;-\"https://www.onemap.gov.sg/api/common/elastic/search\"\n\ncsv&lt;-read_csv(\"data/aspatial/Generalinformationofschools.csv\")\npostcodes&lt;-csv$`postal_code`\n\nfound&lt;-data.frame()\nnot_found&lt;-data.frame()\n\nfor(postcode in postcodes){\n  query&lt;-list('searchVal'=postcode,'returnGeom'='Y','getAddrDetails'='Y','pageNum'='1')\n  res&lt;- GET(url,query=query)\n  \n  if((content(res)$found)!=0){\n    found&lt;-rbind(found,data.frame(content(res))[4:13])\n  } else{\n    not_found = data.frame(postcode)\n  }\n}\n\nNext, the code chunk below will be used to combine both found and not_found data.frames into a single tibble data.frame called merged. At the same time, we will write merged and not_found tibble data.frames into two separate csv files called schools and not_found respectively.\n\nmerged = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)\nwrite.csv(merged, file = \"data/aspatial/schools.csv\")\nwrite.csv(not_found, file = \"data/aspatial/not_found.csv\")\n\n\n\n\n\n\n\nDo it yourself!\n\n\n\n\nWith the help of Google Map, located the location information of the ungeocoded school by using it’s postcode.\nUpdate the results.LATITUDE and results.LONGITUDE fields of the ungeocoded record in schoolss.csv manually.\n\n\n\n\n\nTidying schools data.frame\nIn this sub-section, you will import schools.csv into R environment and at the same time tidying the data by selecting only the necessary fields as well as rename some fields.\n\n\n\n\n\n\nDo it yourself!\n\n\n\nUsing the steps you learned in Hands-on Exercise 1, perform the following tasks:\n\nimport schools.csv in R environment as an tibble data.frame called schools,\nrename results.LATITUDE and results.LONGITUDE to latitude and longitude respectively,\nretain only postal_code, school_name, latitude and longitude in schools tibble data.frame.\n\n\n\n\n\nShow the code chunk\nschools &lt;- read_csv(\"data/aspatial/schools.csv\") %&gt;%\n  rename(latitude = \"results.LATITUDE\",\n         longitude = \"results.LONGITUDE\")%&gt;%\n  select(postal_code, school_name, latitude, longitude)\n\n\n\n\nConverting an aspatial data into sf tibble data.frame\nNext, you will convert schools tibble data.frame data into a simple feature tibble data.frame called schools_sf by using values in latitude and longitude fields.\n\nRefer to st_as_sf() of sf package.\n\n\n\nShow the code chunk\nschools_sf &lt;- st_as_sf(schools, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\nPlotting a point simple feature layer\nTo ensure that schools sf tibble data.frame has been projected and converted correctly, you can plot the schools point data for visual inspection.\nFirst, let us import MPSZ-2019 shapefile into R environment and save it as an sf tibble data.frame called mpsz.\n\n\n\n\n\n\nDo it yourself!\n\n\n\nUsing the step your learned in previous hands-on exercise, import MPSZ-2019 shapefile into R as sf tibble data.frame and name it mpsz.\n\n\n\n\nShow the code chunk\nmpsz &lt;- st_read(dsn = \"data/geospatial/\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nDo it yourself!\nUsing the steps you learned in previous exercises, create a point symbol map showing the location of schools with OSM as the background map.\n\n\n\nShow the code chunk\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz) +\n  tm_polygons() +\ntm_shape(schools_sf) +\n  tm_dots()\n\n\n\n\n\n\n\nPerforming point-in-polygon count process\nNext, we will count the number of schools located inside the planning subzones.\n\n\n\n\n\n\nDo it yourself!\n\n\n\nUsing the steps you learned from previous hands-on exercises, count the number of schools within each planning subzone by using lengths() of Base and st_intersects() of sf package.\n\n\n\n\nShow the code chunk\nmpsz$`SCHOOL_COUNT`&lt;- lengths(\n  st_intersects(\n    mpsz, schools_sf))\n\n\nIt is always a good practice to examine the summary statistics of the derived variable.\n\n\n\n\n\n\nDo it yourself!\n\n\n\nUsing the steps you learned in previous exercises, compute and display the summary statistics of sch_count field.\n\n\n\nsummary(mpsz$SCHOOL_COUNT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   1.054   2.000  12.000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe summary statistics above reveals that there are excessive 0 values in SCHOOL_COUNT field. If log() is going to use to transform this field, additional step is required to ensure that all 0 will be replaced with a value between 0 and 1 but not 0 neither 1."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html#data-integration-and-final-touch-up",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4-GDS.html#data-integration-and-final-touch-up",
    "title": "In-class Exercise 4: Preparing Spatial Interaction Modelling Variables",
    "section": "Data Integration and Final Touch-up",
    "text": "Data Integration and Final Touch-up\n\n\n\n\n\n\nDo it yourself!\n\n\n\nUsing the steps you learned in earlier sub-sections, count the number of Business points in each planning subzone.\n\n\n\nbusiness_sf &lt;- st_read(dsn = \"data/geospatial\",\n                      layer = \"Business\")\n\nReading layer `Business' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6550 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3669.148 ymin: 25408.41 xmax: 47034.83 ymax: 50148.54\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\nShow the code chunk\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz) +\n  tm_polygons() +\ntm_shape(business_sf) +\n  tm_dots()\n\n\n\n\n\n\n\nShow the code chunk\nmpsz$`BUSINESS_COUNT`&lt;- lengths(\n  st_intersects(\n    mpsz, business_sf))\n\n\n\nsummary(mpsz$BUSINESS_COUNT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    2.00   19.73   13.00  307.00 \n\n\nNow, it is time for us to bring in the flow_data.rds saved after Hands-on Exercise 3.\n\n\nShow the code chunk\nflow_data &lt;- read_rds(\"data/rds/flow_data.rds\")\nflow_data\n\n\nSimple feature collection with 14734 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 5105.594 ymin: 25813.33 xmax: 49483.22 ymax: 49552.79\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   ORIGIN_SZ DESTIN_SZ MORNING_PEAK      dist ORIGIN_AGE7_12 ORIGIN_AGE13_24\n1     AMSZ01    AMSZ01         1998   50.0000            310             710\n2     AMSZ01    AMSZ02         8289  810.4491            310             710\n3     AMSZ01    AMSZ03         8971 1360.9294            310             710\n4     AMSZ01    AMSZ04         2252  840.4432            310             710\n5     AMSZ01    AMSZ05         6136 1076.7916            310             710\n6     AMSZ01    AMSZ06         2148  805.2979            310             710\n7     AMSZ01    AMSZ07         1620 1798.7526            310             710\n8     AMSZ01    AMSZ08         1925 2576.0199            310             710\n9     AMSZ01    AMSZ09         1773 1204.2846            310             710\n10    AMSZ01    AMSZ10           63 1417.8035            310             710\n   ORIGIN_AGE25_64 DESTIN_AGE7_12 DESTIN_AGE13_24 DESTIN_AGE25_64\n1             2780         310.00          710.00         2780.00\n2             2780        1140.00         2770.00        15700.00\n3             2780        1010.00         2650.00        14240.00\n4             2780         980.00         2000.00        11320.00\n5             2780         810.00         1920.00         9650.00\n6             2780        1050.00         2390.00        12460.00\n7             2780         420.00         1120.00         3620.00\n8             2780         390.00         1150.00         4350.00\n9             2780        1190.00         3260.00        13350.00\n10            2780           0.99            0.99            0.99\n                         geometry\n1  LINESTRING (29501.77 39419....\n2  LINESTRING (29501.77 39419....\n3  LINESTRING (29501.77 39419....\n4  LINESTRING (29501.77 39419....\n5  LINESTRING (29501.77 39419....\n6  LINESTRING (29501.77 39419....\n7  LINESTRING (29501.77 39419....\n8  LINESTRING (29501.77 39419....\n9  LINESTRING (29501.77 39419....\n10 LINESTRING (29501.77 39419....\n\n\nNotice that this is an sf tibble data.frame and the features are polylines linking the centroid of origins and destination planning subzone.\n\n\n\n\n\n\nDo it yourself\n\n\n\nUsing the steps your learned in Hands-on Exercise 3, append SCHOOL_COUNT and BUSINESS_COUNT into flow_data sf tibble data.frame.\n\n\n\n\nShow the code chunk\nmpsz_tidy &lt;- mpsz %&gt;%\n  st_drop_geometry() %&gt;%\n  select(SUBZONE_C, SCHOOL_COUNT, BUSINESS_COUNT)\n\n\nNow, we will append SCHOOL_COUNT and BUSINESS_COUNT fields from mpsz_tidy data.frame into flow_data sf tibble data.frame by using the code chunk below.\n\nflow_data &lt;- flow_data %&gt;%\n  left_join(mpsz_tidy,\n            by = c(\"DESTIN_SZ\" = \"SUBZONE_C\")) %&gt;%\n  rename(TRIPS = MORNING_PEAK,\n         DIST = dist)\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the unique join field used is DESTIN_SZ of flow_data and SUBZONE_C of mpsz_tidy. Do you know why?\n\n\n\nChecking for variables with zero values\nSince Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.\nIn the code chunk below, summary() of Base R is used to compute the summary statistics of all variables in wd_od data frame.\n\nsummary(flow_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS             DIST      \n Length:14734       Length:14734       Min.   :     1   Min.   :   50  \n Class :character   Class :character   1st Qu.:    14   1st Qu.: 3346  \n Mode  :character   Mode  :character   Median :    76   Median : 6067  \n                                       Mean   :  1021   Mean   : 6880  \n                                       3rd Qu.:   426   3rd Qu.: 9729  \n                                       Max.   :232187   Max.   :26136  \n ORIGIN_AGE7_12    ORIGIN_AGE13_24    ORIGIN_AGE25_64    DESTIN_AGE7_12   \n Min.   :   0.99   Min.   :    0.99   Min.   :    0.99   Min.   :   0.99  \n 1st Qu.: 240.00   1st Qu.:  440.00   1st Qu.: 2200.00   1st Qu.: 240.00  \n Median : 700.00   Median : 1350.00   Median : 6810.00   Median : 720.00  \n Mean   :1031.86   Mean   : 2268.84   Mean   :10487.62   Mean   :1033.40  \n 3rd Qu.:1480.00   3rd Qu.: 3260.00   3rd Qu.:15770.00   3rd Qu.:1500.00  \n Max.   :6340.00   Max.   :16380.00   Max.   :74610.00   Max.   :6340.00  \n DESTIN_AGE13_24    DESTIN_AGE25_64     SCHOOL_COUNT    BUSINESS_COUNT  \n Min.   :    0.99   Min.   :    0.99   Min.   : 0.000   Min.   :  0.00  \n 1st Qu.:  460.00   1st Qu.: 2200.00   1st Qu.: 0.000   1st Qu.:  0.00  \n Median : 1420.00   Median : 7030.00   Median : 1.000   Median :  3.00  \n Mean   : 2290.35   Mean   :10574.46   Mean   : 1.583   Mean   : 16.17  \n 3rd Qu.: 3260.00   3rd Qu.:15830.00   3rd Qu.: 2.000   3rd Qu.: 12.00  \n Max.   :16380.00   Max.   :74610.00   Max.   :12.000   Max.   :307.00  \n          geometry    \n LINESTRING   :14734  \n epsg:3414    :    0  \n +proj=tmer...:    0  \n                      \n                      \n                      \n\n\nThe print report above reveals that variables ORIGIN_AGE7_12, ORIGIN_AGE13_24, ORIGIN_AGE25_64, DESTIN_AGE7_12, DESTIN_AGE13_24, DESTIN_AGE25_64 consist of 0 values.\nIn view of this, code chunk below will be used to replace zero values to 0.99.\n\nflow_data$SCHOOL_COUNT &lt;- ifelse(\n  flow_data$SCHOOL_COUNT == 0,\n  0.99, flow_data$SCHOOL_COUNT)\nflow_data$BUSINESS_COUNT &lt;- ifelse(\n  flow_data$BUSINESS_COUNT == 0,\n  0.99, flow_data$BUSINESS_COUNT)\n\nYou can run the summary() again.\n\nsummary(flow_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS             DIST      \n Length:14734       Length:14734       Min.   :     1   Min.   :   50  \n Class :character   Class :character   1st Qu.:    14   1st Qu.: 3346  \n Mode  :character   Mode  :character   Median :    76   Median : 6067  \n                                       Mean   :  1021   Mean   : 6880  \n                                       3rd Qu.:   426   3rd Qu.: 9729  \n                                       Max.   :232187   Max.   :26136  \n ORIGIN_AGE7_12    ORIGIN_AGE13_24    ORIGIN_AGE25_64    DESTIN_AGE7_12   \n Min.   :   0.99   Min.   :    0.99   Min.   :    0.99   Min.   :   0.99  \n 1st Qu.: 240.00   1st Qu.:  440.00   1st Qu.: 2200.00   1st Qu.: 240.00  \n Median : 700.00   Median : 1350.00   Median : 6810.00   Median : 720.00  \n Mean   :1031.86   Mean   : 2268.84   Mean   :10487.62   Mean   :1033.40  \n 3rd Qu.:1480.00   3rd Qu.: 3260.00   3rd Qu.:15770.00   3rd Qu.:1500.00  \n Max.   :6340.00   Max.   :16380.00   Max.   :74610.00   Max.   :6340.00  \n DESTIN_AGE13_24    DESTIN_AGE25_64     SCHOOL_COUNT    BUSINESS_COUNT  \n Min.   :    0.99   Min.   :    0.99   Min.   : 0.990   Min.   :  0.99  \n 1st Qu.:  460.00   1st Qu.: 2200.00   1st Qu.: 0.990   1st Qu.:  0.99  \n Median : 1420.00   Median : 7030.00   Median : 1.000   Median :  3.00  \n Mean   : 2290.35   Mean   :10574.46   Mean   : 1.987   Mean   : 16.47  \n 3rd Qu.: 3260.00   3rd Qu.:15830.00   3rd Qu.: 2.000   3rd Qu.: 12.00  \n Max.   :16380.00   Max.   :74610.00   Max.   :12.000   Max.   :307.00  \n          geometry    \n LINESTRING   :14734  \n epsg:3414    :    0  \n +proj=tmer...:    0  \n                      \n                      \n                      \n\n\nNotice that all the 0 values have been replaced by 0.99.\nBefore we move on to calibrate the Spatial Interaction Models, let us save flow_data sf tibble data.frame into an rds file. Call the file flow_data_tidy.\n\nwrite_rds(flow_data,\n          \"data/rds/flow_data_tidy.rds\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624",
    "section": "",
    "text": "Welcome to ISSS624 Geospatial Analytics Applications!\nIn this webpage, I am going to share with you my learning journey of geospatial analytics."
  }
]