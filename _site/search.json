[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangling geospatial data using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below install and load sf and tidyverse packages into R environment.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\n\nImporting polygon feature data\n\nmpsz <- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\tskam\\ISSS624\\Hands-on_Ex1\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624",
    "section": "",
    "text": "Welcome to ISSS624 Geospatial Analytics Applications!\nIn this webpage, I am going to share with you my learning journey of geospatial analytics."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_ex1.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "This xxx"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_ex1.html#getting-started",
    "href": "In-class_Ex/In-class_Ex1/In-class_ex1.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nThe code chunk below will install and load tidyverse and sf packages.\n\npacman::p_load(sf, tidyverse, spdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_ex1.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_ex1.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1",
    "section": "3 Importing Geospatial Data",
    "text": "3 Importing Geospatial Data\n\n3.1 Importing polygon features\nThis code chunk will import ESRI shapefile into R.\n\nhunan_sf <- st_read(dsn = \"data/geospatial\",\n                layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.2 Importing attribute data in csv\n\nhunan <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangling geospatial data using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below install and load sf and tidyverse packages into R environment.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\n\nImporting polygon feature data\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\tskam\\ISSS624\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Non-functional Waterpoints in using Geographically Weighted Logistic Regression",
    "section": "",
    "text": "In Chapter 4, you learned how to compute Global and Local Measures of Spatial Association by using functions provided by spdep package. In this chapter, you will gain hands-on experience on using functions provide by sfdep, an interface to integrate with sf objects and the tidyverse framework, to compute Global and Local Measures of Spatial Association."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#getting-started",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#getting-started",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Non-functional Waterpoints in using Geographically Weighted Logistic Regression",
    "section": "Getting Started",
    "text": "Getting Started\nInstalling sfdep\n\nremotes::install_github(\"josiahparry/sfdep\")\n\nIn the code chunk below, p_load() of pacman package is used to install and load the following R packages into R environment:\n\nsf,\ntidyverse,\ntmap,\nspdep, and\nfunModeling will be used for rapid Exploratory Data Analysis\nrgeoda\n\n\npacman::p_load(sf, tidyverse, tmap, corrplot, ggpubr, blorr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-geospatial-data",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Non-functional Waterpoints in using Geographically Weighted Logistic Regression",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\nIn this in-class data, two geospatial data sets will be used, they are:\n\ngeo_export\nnga_admbnda_adm2_osgof_20190417\n\n\nImporting water point geospatial data\nFirst, we are going to import the water point geospatial data (i.e. geo_export) by using the code chunk below.\n\nwp <- st_read(dsn = \"geodata\",\n              layer = \"geo_export\",\n              crs = 4326) %>%\n  filter(clean_coun == \"Nigeria\")\n\nThings to learn from the code chunk above:\n\nst_read() of sf package is used to import geo_export shapefile into R environment and save the imported geospatial data into simple feature data table.\nfilter() of dplyr package is used to extract water point records of Nigeria.\n\n\nBe warned: Avoid performing transformation if you plan to use st_intersects() of sf package in the later stage of the geoprocessing. This is because st_intersects() only works correctly if the geospatial data are in geographic coordinate system (i.e. wgs84)\n\nNext, write_rds() of readr package is used to save the extracted sf data table (i.e. wp) into an output file in rds data format. The output file is called wp_nga.rds and it is saved in geodata sub-folder.\n\nwrite_rds(wp, \"geodata/wp_nga.rds\")\n\n\n\nImporting Nigeria LGA boundary data\nNow, we are going to import the LGA boundary data into R environment by using the code chunk below.\n\nnga <- st_read(dsn = \"geodata\",\n               layer = \"nga_admbnda_adm2_osgof_20190417\",\n               crs = 4326)\n\nThing to learn from the code chunk above.\n\nst_read() of sf package is used to import nga_admbnda_adm2_osgof_20190417 shapefile into R environment and save the imported geospatial data into simple feature data table."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#data-wrangling",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#data-wrangling",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Non-functional Waterpoints in using Geographically Weighted Logistic Regression",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nRecoding NA values into string\nIn the code chunk below, replace_na() is used to recode all the NA values in status_cle field into Unknown.\n\nwp_nga <- read_rds(\"geodata/wp_nga.rds\") %>%\n  mutate(status_cle = replace_na(status_cle, \"Unknown\"))\n\n\n\nEDA\nIn the code chunk below, freq() of funModeling package is used to display the distribution of status_cle field in wp_nga.\n\nfreq(data=wp_nga, \n     input = 'status_cle')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#extracting-water-point-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#extracting-water-point-data",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Non-functional Waterpoints in using Geographically Weighted Logistic Regression",
    "section": "Extracting Water Point Data",
    "text": "Extracting Water Point Data\nIn this section, we will extract the water point records by using classes in status_cle field.\n\nExtracting funtional water point\nIn the code chunk below, filter() of dplyr is used to select functional water points.\n\nwpt_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\", \n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\n\nfreq(data=wpt_functional, \n     input = 'status_cle')\n\n\n\nExtracting non-funtional water point\nIn the code chunk below, filter() of dplyr is used to select non-functional water points.\n\nwpt_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Abandoned/Decommissioned\", \n             \"Abandoned\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\",\n             \"Non-Functional due to dry season\"))\n\n\nfreq(data=wpt_nonfunctional, \n     input = 'status_cle')\n\n\n\nExtracting water point with Unknown class\nIn the code chunk below, filter() of dplyr is used to select water points with unknown status.\n\nwpt_unknown <- wp_nga %>%\n  filter(status_cle == \"Unknown\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#performing-point-in-polygon-count",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#performing-point-in-polygon-count",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Non-functional Waterpoints in using Geographically Weighted Logistic Regression",
    "section": "Performing Point-in-Polygon Count",
    "text": "Performing Point-in-Polygon Count\n\nnga_wp <- nga %>% \n  mutate(`total wpt` = lengths(\n    st_intersects(nga, wp_nga))) %>%\n  mutate(`wpt functional` = lengths(\n    st_intersects(nga, wpt_functional))) %>%\n  mutate(`wpt non-functional` = lengths(\n    st_intersects(nga, wpt_nonfunctional))) %>%\n  mutate(`wpt unknown` = lengths(\n    st_intersects(nga, wpt_unknown)))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#saving-the-analytical-data-table",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#saving-the-analytical-data-table",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Non-functional Waterpoints in using Geographically Weighted Logistic Regression",
    "section": "Saving the Analytical Data Table",
    "text": "Saving the Analytical Data Table\n\nnga_wp <- nga_wp %>%\n  mutate(pct_functional = `wpt functional`/`total wpt`) %>%\n  mutate(`pct_non-functional` = `wpt non-functional`/`total wpt`) %>%\n  select(3:4, 9:10, 18:23)\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr package is used to derive two fields namely pct_functional and pct_non-functional.\nto keep the file size small, select() of dplyr is used to retain only field 3,4,9,10, 18,19,20,21,22,and 23.\n\nNow, you have the tidy sf data table subsequent analysis. We will save the sf data table into rds format.\n\nwrite_rds(nga_wp, \"geodata/nga_wp.rds\")\n\nBefore you end this section, please remember to delete away all the raw data. Notice that the only data file left is nga_wp.rds and it’s file size is aroung 2.1MB.\n\nnga_wp <- read_rds(\"geodata/nga_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#visualising-the-spatial-dsitribution-of-water-points",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#visualising-the-spatial-dsitribution-of-water-points",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Non-functional Waterpoints in using Geographically Weighted Logistic Regression",
    "section": "Visualising the spatial dsitribution of water points",
    "text": "Visualising the spatial dsitribution of water points\n\ntotal <- qtm(nga_wp, \"total wpt\")\nwp_functional <- qtm(nga_wp, \"wpt functional\")\nwp_nonfunctional <- qtm(nga_wp, \"wpt non-functional\")\nunknown <- qtm(nga_wp, \"wpt unknown\")\n\ntmap_arrange(total, wp_functional, wp_nonfunctional, unknown, asp=1, ncol=2)\n\n\n\n\nFirst, we will exclude LGA without water point from the analysis\n\nnga_wp <- nga_wp %>%\n  filter(`total wpt` > 0)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "title": "Regionalisation with Spatially Constrained Cluster Analysis",
    "section": "",
    "text": "This in-class exercise note aims to share with ways to handle aspatial and geospatial data that might be useful when you work on Take-home Exercise 2. By the end of this in-class exercise, you will be able to:\n\nimport geospatial data in wkt format into R,\nconvert the tibble data frame with wkt field into sf data frame, and\nperforming point-in-polygon overlay."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#data-import",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#data-import",
    "title": "Regionalisation with Spatially Constrained Cluster Analysis",
    "section": "Data Import",
    "text": "Data Import\nIn this in-class exercise, two data sets will be used. They are:\n\nImporting water point data\nFirst, we are going to import the water point data into R environment.\n\nwp_nga <- read_csv(\"aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\nThing to learn from the code chunk above:\n\nThe original file name is called Water_Point_Data_Exchange_-_PlusWPdx.csv, it has been rename to WPdx.csv for easy encoding.\nInstead of using read.csv() of Base R to import the csv file into R, read_csv() is readr package is used. This is because during the initial data exploration, we notice that there is at least one field name with space between the field name (ie. New Georeferenced Column)\nThe data file contains water point data of many countries. In this study, we are interested on water point in Nigeria on. Hence, filter() of dplyr is used to extract out records belong to Nigeria only.\n\n\nConvert wkt data\nAfter the data are imported into R environment, it is a good practice to review both the data structure and the data table if it is in tibble data frame format in R Studio.\nNotice that the newly imported tibble data frame (i.e. wp_nga) contains a field called New Georeferenced Column which represent spatial data in a textual format. In fact, this kind of text file is popularly known as Well Known Text in short wkt.\n\nTwo steps will be used to convert an asptial data file in wkt format into a sf data frame by using sf.\nFirst, st_as_sfc() of sf package is used to derive a new field called Geometry as shown in the code chunk below.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\n\nIf you open wp_nga data frame and scroll to the last field now, you will see a new field called Geometry has been added as shown below.\n\nNext, st_sf() will be used to convert the tibble data frame into sf data frame.\n\nwp_sf <- st_sf(wp_nga, crs=4326) \n\nWhen the process completed, a new sf data frame called wp_sf will be created.\n\n\n\n\nImporting Nigeria LGA level boundary data\nFor the purpose of this exercise, shapefile downloaded from geoBoundaries portal will be used.\n\nnga <- st_read(dsn = \"geospatial\",\n               layer = \"geoBoundaries-NGA-ADM2\",\n               crs = 4326) %>%\n  select(shapeName)\n\n\nNote: One of your classmate had done an excellance job in cleaning the LGA data before subsequent processing and analysis. You are encouraged to refer to his Take-home Exercise 1 page especially the sub-section on Data wrangling.\n\n\nGentle reminder: Please remember to acknowledge your classmate work if you referred to his procedures and code chunks."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#read-csv-files",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#read-csv-files",
    "title": "Regionalisation with Spatially Constrained Cluster Analysis",
    "section": "read CSV files",
    "text": "read CSV files\nFirst, we are going to import the water point data into R environment.\n\nwp_nga <- read_csv(\"aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\nThing to learn from the code chunk above:\n\nThe original file name is called Water_Point_Data_Exchange_-_PlusWPdx.csv, it has been rename to WPdx.csv for easy encoding.\nInstead of using read.csv() of Base R to import the csv file into R, read_csv() is readr package is used. This is because during the initial data exploration, we notice that there is at least one field name with space between the field name (ie. New Georeferenced Column)\nThe data file contains water point data of many countries. In this study, we are interested on water point in Nigeria on. Hence, filter() of dplyr is used to extract out records belong to Nigeria only.\n\nConvert the column “geometry” to sfc\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\nwp_nga"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#getting-started",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#getting-started",
    "title": "Regionalisation with Spatially Constrained Cluster Analysis",
    "section": "Getting Started",
    "text": "Getting Started\nAs usual, to get started, we will load the necessary R packages. For the purpose of this in-class exercise, three R packages will be used, they are:\n\nsf for importing and processing geospatial data,\ntidyverse for importing and processing non-spatial data. In this exercise, readr package will be used for importing wkt data and dplyr package will be used to wrangling the data.\n\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#point-in-polygon-overlay",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#point-in-polygon-overlay",
    "title": "Regionalisation with Spatially Constrained Cluster Analysis",
    "section": "Point in Polygon Overlay",
    "text": "Point in Polygon Overlay\nAlthough wp_sf sf data frame consists of a field called #clean_adm2 which by right should provides the LGA name of the water point located. However, it is always a good practice to be more caution when dealing with data accuracy.\nIn this section, we are going to use a geoprocessing function (or commonly know as GIS analysis) called point-in-polygon overlay to transfer the attribute information in nga sf data frame into wp_sf data frame.\n\nwp_sf <- st_join(wp_sf, nga)\n\nNotice that a new field called shapeName has been added into wp_sf sf data frame. as shown below.\n\nNow you will be able to apply appropriate data wrangling functions of dplyr to extract the necessary clustering variables by using this data frame.\n\nBe warned: Please do remember to delete the large data files in the project repository before push the changes onto github."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on building an explanatory model for water pump status (function/non-functional) by using Geographically Weighted Logistic Regression (GWLR) method of GWModel. By the end of this in-class exercise, you will be able to:\n\nimport geospatial data in wkt format into R,\nconvert the tibble data frame with wkt field into sf data frame, and\nperforming point-in-polygon overlay."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#getting-started",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#getting-started",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "Getting Started",
    "text": "Getting Started\nAs usual, to get started, we will load the necessary R packages. For the purpose of this in-class exercise, three R packages will be used, they are:\n\nsf for importing and processing geospatial data,\ntidyverse for importing and processing non-spatial data. In this exercise, readr package will be used for importing wkt data and dplyr package will be used to wrangling the data.\n\n\npacman::p_load(sf, tidyverse, funModeling, blorr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, skimr, caret)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#data-import",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#data-import",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "Data Import",
    "text": "Data Import\nIn this in-class exercise, two data sets will be used. They are:\n\nImporting water point data\nFirst, we are going to import the water point data into R environment.\n\nwp_nga <- read_csv(\"aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\nglimpse(wp_nga)\n\n\ncolnames(wp_nga) <- gsub(\"#\", \"\", names(wp_nga), fixed = TRUE)\n\nThing to learn from the code chunk above:\n\nThe original file name is called Water_Point_Data_Exchange_-_PlusWPdx.csv, it has been rename to WPdx.csv for easy encoding.\nInstead of using read.csv() of Base R to import the csv file into R, read_csv() is readr package is used. This is because during the initial data exploration, we notice that there is at least one field name with space between the field name (ie. New Georeferenced Column)\nThe data file contains water point data of many countries. In this study, we are interested on water point in Nigeria on. Hence, filter() of dplyr is used to extract out records belong to Nigeria only.\n\n\nConvert wkt data\nAfter the data are imported into R environment, it is a good practice to review both the data structure and the data table if it is in tibble data frame format in R Studio.\nNotice that the newly imported tibble data frame (i.e. wp_nga) contains a field called New Georeferenced Column which represent spatial data in a textual format. In fact, this kind of text file is popularly known as Well Known Text in short wkt.\n\nTwo steps will be used to convert an asptial data file in wkt format into a sf data frame by using sf.\nFirst, st_as_sfc() of sf package is used to derive a new field called Geometry as shown in the code chunk below.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\n\nIf you open wp_nga data frame and scroll to the last field now, you will see a new field called Geometry has been added as shown below.\n\nNext, st_sf() will be used to convert the tibble data frame into sf data frame.\n\nwp_sf <- st_sf(wp_nga, crs=4326) %>%\n  st_transform(crs = 26392)\nwp_sf\n\nWhen the process completed, a new sf data frame called wp_sf will be created.\n\n\n\n\nImporting Nigeria LGA level boundary data\nFor the purpose of this exercise, shapefile downloaded from geoBoundaries portal will be used.\n\nnga <- st_read(dsn = \"geodata\",\n               layer = \"HDX_AMD2_2019\",\n               crs = 4326) %>% \n  st_transform(crs = 26392) %>%\n  select(c(3:4,8:9))\nnga\n\n\nNote: One of your classmate had done an excellance job in cleaning the LGA data before subsequent processing and analysis. You are encouraged to refer to his Take-home Exercise 1 page especially the sub-section on Data wrangling.\n\n\nGentle reminder: Please remember to acknowledge your classmate work if you referred to his procedures and code chunks."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#point-in-polygon-overlay",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#point-in-polygon-overlay",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "Point in Polygon Overlay",
    "text": "Point in Polygon Overlay\nAlthough wp_sf sf data frame consists of a field called #clean_adm2 which by right should provides the LGA name of the water point located. However, it is always a good practice to be more caution when dealing with data accuracy.\nIn this section, we are going to use a geoprocessing function (or commonly know as GIS analysis) called point-in-polygon overlay to transfer the attribute information in nga sf data frame into wp_sf data frame.\n\nwp_sf <- st_join(wp_sf, nga)\n\nNotice that a new field called shapeName has been added into wp_sf sf data frame. as shown below.\n\nNow you will be able to apply appropriate data wrangling functions of dplyr to extract the necessary clustering variables by using this data frame.\n\nBe warned: Please do remember to delete the large data files in the project repository before push the changes onto github.\n\n\nDefining the binary response variable\nFor binary logistic regression model, the response variable (also known as dependent variable) should be encoded in binary form, namely 1=TRUE, 0=FALSE. In the code chunk below,\n\nOsun_wp_sf <- wp_sf %>% \n  mutate(`status_clean` = replace_na(`status_clean`, \"Unknown\")) %>%\n  filter(`status_clean` != \"Unknown\") %>%\n  mutate(`status` = recode(`status_clean`,\n    \"Functional\" = T, \n    \"Functional but not in use\" = T,\n    \"Functional but needs repair\" = T,\n    \"Abandoned/Decommissioned\" = F, \n             \"Abandoned\" = F,\n             \"Non-Functional\" = F,\n             \"Non functional due to dry season\" = F,\n             \"Non-Functional due to dry season\" = F))  %>% \nfilter(ADM1_EN == \"Osun\")\nwrite_rds(Osun_wp_sf, \"rds/Osun_wp_sf.rds\")\n\nThings to learn from the code chunk above.\n\nmutate() and replace_na() is used to replace NA values in status_clean field with Unknown\nfilter() is used to exclude\n\nExtract Osun State boundary layer\n\nOsun <- nga %>%\n  filter(ADM1_EN == \"Osun\") \nwrite_rds(Osun, \"rds/Osun.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#importing-the-analytical-data",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#importing-the-analytical-data",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "Importing the Analytical Data",
    "text": "Importing the Analytical Data\n\nOsun <- read_rds(\"rds/Osun.rds\")\nOsun_wp_sf <- read_rds(\"rds/Osun_wp_sf.rds\")\n\n\nOsun_wp_sf %>% \n  freq(input = 'status')\n\n\n\n\n  status frequency percentage cumulative_perc\n1   TRUE      2642       55.5            55.5\n2  FALSE      2118       44.5           100.0\n\n\n\ntmap_mode(\"view\")\ntm_shape(Osun)+\n#  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(Osun_wp_sf) +  \n  tm_dots(col = \"status\",\n          alpha = 0.6) +\n  tm_view(set.zoom.limits = c(9,12))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#exploratory-data-analysis",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#exploratory-data-analysis",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nSummary Statistics with skimr\n\nOsun_wp_sf %>%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n4760\n\n\nNumber of columns\n75\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n47\n\n\nlogical\n5\n\n\nnumeric\n23\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsource\n0\n1.00\n5\n44\n0\n2\n0\n\n\nreport_date\n0\n1.00\n22\n22\n0\n42\n0\n\n\nstatus_id\n0\n1.00\n2\n7\n0\n3\n0\n\n\nwater_source_clean\n0\n1.00\n8\n22\n0\n3\n0\n\n\nwater_source_category\n0\n1.00\n4\n6\n0\n2\n0\n\n\nwater_tech_clean\n24\n0.99\n9\n23\n0\n3\n0\n\n\nwater_tech_category\n24\n0.99\n9\n15\n0\n2\n0\n\n\nfacility_type\n0\n1.00\n8\n8\n0\n1\n0\n\n\nclean_country_name\n0\n1.00\n7\n7\n0\n1\n0\n\n\nclean_adm1\n0\n1.00\n3\n5\n0\n5\n0\n\n\nclean_adm2\n0\n1.00\n3\n14\n0\n35\n0\n\n\nclean_adm3\n4760\n0.00\nNA\nNA\n0\n0\n0\n\n\nclean_adm4\n4760\n0.00\nNA\nNA\n0\n0\n0\n\n\ninstaller\n4760\n0.00\nNA\nNA\n0\n0\n0\n\n\nmanagement_clean\n1573\n0.67\n5\n37\n0\n7\n0\n\n\nstatus_clean\n0\n1.00\n9\n32\n0\n7\n0\n\n\npay\n0\n1.00\n2\n39\n0\n7\n0\n\n\nfecal_coliform_presence\n4760\n0.00\nNA\nNA\n0\n0\n0\n\n\nsubjective_quality\n0\n1.00\n18\n20\n0\n4\n0\n\n\nactivity_id\n4757\n0.00\n36\n36\n0\n3\n0\n\n\nscheme_id\n4760\n0.00\nNA\nNA\n0\n0\n0\n\n\nwpdx_id\n0\n1.00\n12\n12\n0\n4760\n0\n\n\nnotes\n0\n1.00\n2\n96\n0\n3502\n0\n\n\norig_lnk\n4757\n0.00\n84\n84\n0\n1\n0\n\n\nphoto_lnk\n41\n0.99\n84\n84\n0\n4719\n0\n\n\ncountry_id\n0\n1.00\n2\n2\n0\n1\n0\n\n\ndata_lnk\n0\n1.00\n79\n96\n0\n2\n0\n\n\nwater_point_history\n0\n1.00\n142\n834\n0\n4750\n0\n\n\nclean_country_id\n0\n1.00\n3\n3\n0\n1\n0\n\n\ncountry_name\n0\n1.00\n7\n7\n0\n1\n0\n\n\nwater_source\n0\n1.00\n8\n30\n0\n4\n0\n\n\nwater_tech\n0\n1.00\n5\n37\n0\n20\n0\n\n\nadm2\n0\n1.00\n3\n14\n0\n33\n0\n\n\nadm3\n4760\n0.00\nNA\nNA\n0\n0\n0\n\n\nmanagement\n1573\n0.67\n5\n47\n0\n7\n0\n\n\nadm1\n0\n1.00\n4\n5\n0\n4\n0\n\n\nNew Georeferenced Column\n0\n1.00\n16\n35\n0\n4760\n0\n\n\nlat_lon_deg\n0\n1.00\n13\n32\n0\n4760\n0\n\n\npublic_data_source\n0\n1.00\n84\n102\n0\n2\n0\n\n\nconverted\n0\n1.00\n53\n53\n0\n1\n0\n\n\ncreated_timestamp\n0\n1.00\n22\n22\n0\n2\n0\n\n\nupdated_timestamp\n0\n1.00\n22\n22\n0\n2\n0\n\n\nGeometry\n0\n1.00\n33\n37\n0\n4760\n0\n\n\nADM2_EN\n0\n1.00\n3\n14\n0\n30\n0\n\n\nADM2_PCODE\n0\n1.00\n8\n8\n0\n30\n0\n\n\nADM1_EN\n0\n1.00\n4\n4\n0\n1\n0\n\n\nADM1_PCODE\n0\n1.00\n5\n5\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nrehab_year\n4760\n0\nNaN\n:\n\n\nrehabilitator\n4760\n0\nNaN\n:\n\n\nis_urban\n0\n1\n0.39\nFAL: 2884, TRU: 1876\n\n\nlatest_record\n0\n1\n1.00\nTRU: 4760\n\n\nstatus\n0\n1\n0.56\nTRU: 2642, FAL: 2118\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrow_id\n0\n1.00\n68550.48\n10216.94\n49601.00\n66874.75\n68244.50\n69562.25\n471319.00\n▇▁▁▁▁\n\n\nlat_deg\n0\n1.00\n7.68\n0.22\n7.06\n7.51\n7.71\n7.88\n8.06\n▁▂▇▇▇\n\n\nlon_deg\n0\n1.00\n4.54\n0.21\n4.08\n4.36\n4.56\n4.71\n5.06\n▃▆▇▇▂\n\n\ninstall_year\n1144\n0.76\n2008.63\n6.04\n1917.00\n2006.00\n2010.00\n2013.00\n2015.00\n▁▁▁▁▇\n\n\nfecal_coliform_value\n4760\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ndistance_to_primary_road\n0\n1.00\n5021.53\n5648.34\n0.01\n719.36\n2972.78\n7314.73\n26909.86\n▇▂▁▁▁\n\n\ndistance_to_secondary_road\n0\n1.00\n3750.47\n3938.63\n0.15\n460.90\n2554.25\n5791.94\n19559.48\n▇▃▁▁▁\n\n\ndistance_to_tertiary_road\n0\n1.00\n1259.28\n1680.04\n0.02\n121.25\n521.77\n1834.42\n10966.27\n▇▂▁▁▁\n\n\ndistance_to_city\n0\n1.00\n16663.99\n10960.82\n53.05\n7930.75\n15030.41\n24255.75\n47934.34\n▇▇▆▃▁\n\n\ndistance_to_town\n0\n1.00\n16726.59\n12452.65\n30.00\n6876.92\n12204.53\n27739.46\n44020.64\n▇▅▃▃▂\n\n\nrehab_priority\n2654\n0.44\n489.33\n1658.81\n0.00\n7.00\n91.50\n376.25\n29697.00\n▇▁▁▁▁\n\n\nwater_point_population\n4\n1.00\n513.58\n1458.92\n0.00\n14.00\n119.00\n433.25\n29697.00\n▇▁▁▁▁\n\n\nlocal_population_1km\n4\n1.00\n2727.16\n4189.46\n0.00\n176.00\n1032.00\n3717.00\n36118.00\n▇▁▁▁▁\n\n\ncrucialness_score\n798\n0.83\n0.26\n0.28\n0.00\n0.07\n0.15\n0.35\n1.00\n▇▃▁▁▁\n\n\npressure_score\n798\n0.83\n1.46\n4.16\n0.00\n0.12\n0.41\n1.24\n93.69\n▇▁▁▁▁\n\n\nusage_capacity\n0\n1.00\n560.74\n338.46\n300.00\n300.00\n300.00\n1000.00\n1000.00\n▇▁▁▁▅\n\n\ndays_since_report\n0\n1.00\n2692.69\n41.92\n1483.00\n2688.00\n2693.00\n2700.00\n4645.00\n▁▇▁▁▁\n\n\nstaleness_score\n0\n1.00\n42.80\n0.58\n23.13\n42.70\n42.79\n42.86\n62.66\n▁▁▇▁▁\n\n\nlocation_id\n0\n1.00\n235865.49\n6657.60\n23741.00\n230638.75\n236199.50\n240061.25\n267454.00\n▁▁▁▁▇\n\n\ncluster_size\n0\n1.00\n1.05\n0.25\n1.00\n1.00\n1.00\n1.00\n4.00\n▇▁▁▁▁\n\n\nlat_deg_original\n4760\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nlon_deg_original\n4760\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ncount\n0\n1.00\n1.00\n0.00\n1.00\n1.00\n1.00\n1.00\n1.00\n▁▁▇▁▁\n\n\n\n\n\n\nOsun_wp_sf_clean <- Osun_wp_sf %>%\n   filter_at(vars(status, \n                  distance_to_primary_road, \n                  distance_to_secondary_road, \n                  distance_to_tertiary_road, \n                  distance_to_city, \n                  distance_to_town, \n                  water_point_population,  \n                  local_population_1km,\n                  usage_capacity,\n                  is_urban, \n                  water_source_clean), \n             all_vars(!is.na(.))) %>%\n  mutate(usage_capacity = as.factor(usage_capacity))\n\nThings to learn from code chunk above.\n\nall_vars(is.na(.)) is used to exclude records with missing values. This is because logistic regression does not accept records with missing values.\n\n\nOsun_wp_sf_clean %>% \n  freq(input = 'status')\n\n\n\n\n  status frequency percentage cumulative_perc\n1   TRUE      2642      55.55           55.55\n2  FALSE      2114      44.45          100.00\n\n\n\nOsun_wp_sf_clean %>%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n4756\n\n\nNumber of columns\n75\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n47\n\n\nfactor\n1\n\n\nlogical\n5\n\n\nnumeric\n22\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsource\n0\n1.00\n5\n44\n0\n2\n0\n\n\nreport_date\n0\n1.00\n22\n22\n0\n42\n0\n\n\nstatus_id\n0\n1.00\n2\n7\n0\n3\n0\n\n\nwater_source_clean\n0\n1.00\n8\n22\n0\n3\n0\n\n\nwater_source_category\n0\n1.00\n4\n6\n0\n2\n0\n\n\nwater_tech_clean\n23\n1.00\n9\n23\n0\n3\n0\n\n\nwater_tech_category\n23\n1.00\n9\n15\n0\n2\n0\n\n\nfacility_type\n0\n1.00\n8\n8\n0\n1\n0\n\n\nclean_country_name\n0\n1.00\n7\n7\n0\n1\n0\n\n\nclean_adm1\n0\n1.00\n3\n5\n0\n5\n0\n\n\nclean_adm2\n0\n1.00\n3\n14\n0\n35\n0\n\n\nclean_adm3\n4756\n0.00\nNA\nNA\n0\n0\n0\n\n\nclean_adm4\n4756\n0.00\nNA\nNA\n0\n0\n0\n\n\ninstaller\n4756\n0.00\nNA\nNA\n0\n0\n0\n\n\nmanagement_clean\n1569\n0.67\n5\n37\n0\n7\n0\n\n\nstatus_clean\n0\n1.00\n9\n32\n0\n6\n0\n\n\npay\n0\n1.00\n2\n39\n0\n7\n0\n\n\nfecal_coliform_presence\n4756\n0.00\nNA\nNA\n0\n0\n0\n\n\nsubjective_quality\n0\n1.00\n18\n20\n0\n4\n0\n\n\nactivity_id\n4753\n0.00\n36\n36\n0\n3\n0\n\n\nscheme_id\n4756\n0.00\nNA\nNA\n0\n0\n0\n\n\nwpdx_id\n0\n1.00\n12\n12\n0\n4756\n0\n\n\nnotes\n0\n1.00\n2\n96\n0\n3499\n0\n\n\norig_lnk\n4753\n0.00\n84\n84\n0\n1\n0\n\n\nphoto_lnk\n41\n0.99\n84\n84\n0\n4715\n0\n\n\ncountry_id\n0\n1.00\n2\n2\n0\n1\n0\n\n\ndata_lnk\n0\n1.00\n79\n96\n0\n2\n0\n\n\nwater_point_history\n0\n1.00\n142\n834\n0\n4746\n0\n\n\nclean_country_id\n0\n1.00\n3\n3\n0\n1\n0\n\n\ncountry_name\n0\n1.00\n7\n7\n0\n1\n0\n\n\nwater_source\n0\n1.00\n8\n30\n0\n4\n0\n\n\nwater_tech\n0\n1.00\n5\n37\n0\n19\n0\n\n\nadm2\n0\n1.00\n3\n14\n0\n33\n0\n\n\nadm3\n4756\n0.00\nNA\nNA\n0\n0\n0\n\n\nmanagement\n1569\n0.67\n5\n47\n0\n7\n0\n\n\nadm1\n0\n1.00\n4\n5\n0\n4\n0\n\n\nNew Georeferenced Column\n0\n1.00\n16\n35\n0\n4756\n0\n\n\nlat_lon_deg\n0\n1.00\n13\n32\n0\n4756\n0\n\n\npublic_data_source\n0\n1.00\n84\n102\n0\n2\n0\n\n\nconverted\n0\n1.00\n53\n53\n0\n1\n0\n\n\ncreated_timestamp\n0\n1.00\n22\n22\n0\n2\n0\n\n\nupdated_timestamp\n0\n1.00\n22\n22\n0\n2\n0\n\n\nGeometry\n0\n1.00\n33\n37\n0\n4756\n0\n\n\nADM2_EN\n0\n1.00\n3\n14\n0\n30\n0\n\n\nADM2_PCODE\n0\n1.00\n8\n8\n0\n30\n0\n\n\nADM1_EN\n0\n1.00\n4\n4\n0\n1\n0\n\n\nADM1_PCODE\n0\n1.00\n5\n5\n0\n1\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nusage_capacity\n0\n1\nFALSE\n2\n300: 2986, 100: 1770\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nrehab_year\n4756\n0\nNaN\n:\n\n\nrehabilitator\n4756\n0\nNaN\n:\n\n\nis_urban\n0\n1\n0.39\nFAL: 2882, TRU: 1874\n\n\nlatest_record\n0\n1\n1.00\nTRU: 4756\n\n\nstatus\n0\n1\n0.56\nTRU: 2642, FAL: 2114\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrow_id\n0\n1.00\n68551.03\n10221.05\n49601.00\n66875.75\n68244.50\n69562.25\n471319.00\n▇▁▁▁▁\n\n\nlat_deg\n0\n1.00\n7.68\n0.22\n7.06\n7.51\n7.71\n7.88\n8.06\n▁▂▇▇▇\n\n\nlon_deg\n0\n1.00\n4.54\n0.21\n4.08\n4.36\n4.56\n4.71\n5.06\n▃▆▇▇▂\n\n\ninstall_year\n1143\n0.76\n2008.63\n6.04\n1917.00\n2006.00\n2010.00\n2013.00\n2015.00\n▁▁▁▁▇\n\n\nfecal_coliform_value\n4756\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ndistance_to_primary_road\n0\n1.00\n5021.73\n5650.02\n0.01\n719.36\n2968.38\n7314.73\n26909.86\n▇▂▁▁▁\n\n\ndistance_to_secondary_road\n0\n1.00\n3751.00\n3939.74\n0.15\n460.50\n2554.25\n5791.94\n19559.48\n▇▃▁▁▁\n\n\ndistance_to_tertiary_road\n0\n1.00\n1259.65\n1680.52\n0.02\n121.33\n521.77\n1834.42\n10966.27\n▇▂▁▁▁\n\n\ndistance_to_city\n0\n1.00\n16662.78\n10961.08\n53.05\n7930.75\n15020.40\n24255.75\n47934.34\n▇▇▆▃▁\n\n\ndistance_to_town\n0\n1.00\n16732.33\n12455.76\n30.00\n6876.92\n12215.09\n27745.52\n44020.64\n▇▅▃▃▂\n\n\nrehab_priority\n2650\n0.44\n489.33\n1658.81\n0.00\n7.00\n91.50\n376.25\n29697.00\n▇▁▁▁▁\n\n\nwater_point_population\n0\n1.00\n513.58\n1458.92\n0.00\n14.00\n119.00\n433.25\n29697.00\n▇▁▁▁▁\n\n\nlocal_population_1km\n0\n1.00\n2727.16\n4189.46\n0.00\n176.00\n1032.00\n3717.00\n36118.00\n▇▁▁▁▁\n\n\ncrucialness_score\n794\n0.83\n0.26\n0.28\n0.00\n0.07\n0.15\n0.35\n1.00\n▇▃▁▁▁\n\n\npressure_score\n794\n0.83\n1.46\n4.16\n0.00\n0.12\n0.41\n1.24\n93.69\n▇▁▁▁▁\n\n\ndays_since_report\n0\n1.00\n2692.69\n41.94\n1483.00\n2688.00\n2693.00\n2700.00\n4645.00\n▁▇▁▁▁\n\n\nstaleness_score\n0\n1.00\n42.80\n0.58\n23.13\n42.70\n42.79\n42.86\n62.66\n▁▁▇▁▁\n\n\nlocation_id\n0\n1.00\n235864.87\n6659.44\n23741.00\n230638.75\n236198.50\n240062.25\n267454.00\n▁▁▁▁▇\n\n\ncluster_size\n0\n1.00\n1.05\n0.25\n1.00\n1.00\n1.00\n1.00\n4.00\n▇▁▁▁▁\n\n\nlat_deg_original\n4756\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nlon_deg_original\n4756\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ncount\n0\n1.00\n1.00\n0.00\n1.00\n1.00\n1.00\n1.00\n1.00\n▁▁▇▁▁"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#building-geographically-weighted-logistic-regression-models",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#building-geographically-weighted-logistic-regression-models",
    "title": "Regionalisation with Spatially Constrained Cluster Analysis",
    "section": "Building Geographically Weighted Logistic Regression Models",
    "text": "Building Geographically Weighted Logistic Regression Models\n\nBuilding Fixed Bandwidth GWR Model\n\nComputing fixed bandwidth\n\nbw.fixed <- bw.ggwr(status ~  distance_to_primary_road +\n                      distance_to_secondary_road + \n                      distance_to_tertiary_road + \n                      distance_to_city + \n                      distance_to_town + \n                      water_point_population + \n                      local_population_1km +\n                      is_urban +\n                      usage_capacity +\n                      water_source_clean, \n                    data=Osun_wp_sp, \n                    family = \"binomial\", \n                    approach =\"AIC\", \n                    kernel =\"gaussian\", \n                    adaptive = FALSE, \n                    longlat =FALSE)\n\n\nbw.fixed\n\n\ngwlr.fixed <- ggwr.basic(status ~  distance_to_primary_road +\n                           distance_to_secondary_road +\n                           distance_to_tertiary_road + \n                           distance_to_city + \n                           distance_to_town + \n                           water_point_population + \n                           local_population_1km +\n                           is_urban + \n                           usage_capacity + \n                           water_source_clean,\n                         data=Osun_wp_sp, \n                         bw = 2597.255, \n                         family = \"binomial\", \n                         kernel =\"gaussian\", \n                         adaptive = FALSE, \n                         longlat =FALSE)\n\n Iteration    Log-Likelihood\n=========================\n       0        -1957 \n       1        -1675 \n       2        -1525 \n       3        -1441 \n       4        -1403 \n       5        -1403 \n\n\n\ngwlr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-12 23:26:47 \n   Call:\n   ggwr.basic(formula = status ~ distance_to_primary_road + distance_to_secondary_road + \n    distance_to_tertiary_road + distance_to_city + distance_to_town + \n    water_point_population + local_population_1km + is_urban + \n    usage_capacity + water_source_clean, data = Osun_wp_sp, bw = 2597.255, \n    family = \"binomial\", kernel = \"gaussian\", adaptive = FALSE, \n    longlat = FALSE)\n\n   Dependent (y) variable:  status\n   Independent variables:  distance_to_primary_road distance_to_secondary_road distance_to_tertiary_road distance_to_city distance_to_town water_point_population local_population_1km is_urban usage_capacity water_source_clean\n   Number of data points: 4756\n   Used family: binomial\n   ***********************************************************************\n   *              Results of Generalized linear Regression               *\n   ***********************************************************************\n\nCall:\nNULL\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-124.555    -1.755     1.072     1.742    34.333  \n\nCoefficients:\n                                           Estimate Std. Error z value Pr(>|z|)\nIntercept                                 3.887e-01  1.124e-01   3.459 0.000543\ndistance_to_primary_road                 -4.642e-06  6.490e-06  -0.715 0.474422\ndistance_to_secondary_road               -5.143e-06  9.299e-06  -0.553 0.580230\ndistance_to_tertiary_road                 9.683e-05  2.073e-05   4.671 3.00e-06\ndistance_to_city                         -1.686e-05  3.544e-06  -4.757 1.96e-06\ndistance_to_town                         -1.480e-05  3.009e-06  -4.917 8.79e-07\nwater_point_population                   -5.097e-04  4.484e-05 -11.369  < 2e-16\nlocal_population_1km                      3.451e-04  1.788e-05  19.295  < 2e-16\nis_urbanTRUE                             -2.971e-01  8.185e-02  -3.629 0.000284\nusage_capacity1000                       -6.230e-01  6.972e-02  -8.937  < 2e-16\nwater_source_cleanProtected Shallow Well  5.040e-01  8.574e-02   5.878 4.14e-09\nwater_source_cleanProtected Spring        1.288e+00  4.388e-01   2.936 0.003325\n                                            \nIntercept                                ***\ndistance_to_primary_road                    \ndistance_to_secondary_road                  \ndistance_to_tertiary_road                ***\ndistance_to_city                         ***\ndistance_to_town                         ***\nwater_point_population                   ***\nlocal_population_1km                     ***\nis_urbanTRUE                             ***\nusage_capacity1000                       ***\nwater_source_cleanProtected Shallow Well ***\nwater_source_cleanProtected Spring       ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6534.5  on 4755  degrees of freedom\nResidual deviance: 5688.0  on 4744  degrees of freedom\nAIC: 5712\n\nNumber of Fisher Scoring iterations: 5\n\n\n AICc:  5712.099\n Pseudo R-square value:  0.1295351\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 2597.255 \n   Regression points: the same locations as observations are used.\n   Distance metric: A distance matrix is specified for this model calibration.\n\n   ************Summary of Generalized GWR coefficient estimates:**********\n                                                   Min.     1st Qu.      Median\n   Intercept                                -8.9630e+02 -4.9805e+00  1.7599e+00\n   distance_to_primary_road                 -1.9477e-02 -4.8092e-04  3.0174e-05\n   distance_to_secondary_road               -1.5757e-02 -3.7583e-04  1.2438e-04\n   distance_to_tertiary_road                -1.5673e-02 -4.2538e-04  7.6217e-05\n   distance_to_city                         -1.8447e-02 -5.6287e-04 -1.2745e-04\n   distance_to_town                         -2.2450e-02 -5.7335e-04 -1.5218e-04\n   water_point_population                   -5.2830e-02 -2.2810e-03 -9.8829e-04\n   local_population_1km                     -1.2757e-01  5.0016e-04  1.0647e-03\n   is_urbanTRUE                             -1.9866e+02 -4.3054e+00 -1.6908e+00\n   usage_capacity1000                       -2.0846e+01 -9.7311e-01 -4.1596e-01\n   water_source_cleanProtected.Shallow.Well -2.0782e+01 -4.5536e-01  5.3278e-01\n   water_source_cleanProtected.Spring       -5.2495e+02 -5.5983e+00  2.5500e+00\n                                                3rd Qu.      Max.\n   Intercept                                 1.2829e+01 1075.4234\n   distance_to_primary_road                  4.8497e-04    0.0143\n   distance_to_secondary_road                6.0665e-04    0.0259\n   distance_to_tertiary_road                 6.7104e-04    0.0129\n   distance_to_city                          2.3763e-04    0.0155\n   distance_to_town                          1.9318e-04    0.0225\n   water_point_population                    5.0564e-04    0.1313\n   local_population_1km                      1.8177e-03    0.0392\n   is_urbanTRUE                              1.2864e+00  746.9498\n   usage_capacity1000                        3.0334e-01    5.9492\n   water_source_cleanProtected.Shallow.Well  1.7870e+00   67.5549\n   water_source_cleanProtected.Spring        6.7736e+00  331.1243\n   ************************Diagnostic information*************************\n   Number of data points: 4756 \n   GW Deviance: 2792.323 \n   AIC : 4413.603 \n   AICc : 4747.217 \n   Pseudo R-square value:  0.5726785 \n\n   ***********************************************************************\n   Program stops at: 2022-12-12 23:27:42 \n\n\n\n\n\nModel Assessment\n\nConverting SDF into sf data.frame\nTo assess the performance of the gw Logistic Regression, firstly, we will convert the SDF object in as data frame by using the code chunk below.\n\ngwr.fixed <- as.data.frame(gwlr.fixed$SDF)\n\nNext, we will label yhat values greater or equal to 0.5 into 1 and else 0. The result of the logic comparison operation will be saved into a field called most.\n\ngwr.fixed <- gwr.fixed %>%\n  mutate(most = ifelse(\n    gwr.fixed$yhat >= 0.5, T, F)) \n\n\ngwr.fixed$y <- as.factor(gwr.fixed$y)\ngwr.fixed$most <- as.factor(gwr.fixed$most)\nCM <- confusionMatrix(data=gwr.fixed$most, reference = gwr.fixed$y)\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE  1824  263\n     TRUE    290 2379\n                                          \n               Accuracy : 0.8837          \n                 95% CI : (0.8743, 0.8927)\n    No Information Rate : 0.5555          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.7642          \n                                          \n Mcnemar's Test P-Value : 0.2689          \n                                          \n            Sensitivity : 0.8628          \n            Specificity : 0.9005          \n         Pos Pred Value : 0.8740          \n         Neg Pred Value : 0.8913          \n             Prevalence : 0.4445          \n         Detection Rate : 0.3835          \n   Detection Prevalence : 0.4388          \n      Balanced Accuracy : 0.8816          \n                                          \n       'Positive' Class : FALSE           \n                                          \n\n\n\n\n\nVisualising gwrLogistic\n\nOsun_wp_sf_selected <- Osun_wp_sf_clean %>% \n  select(c(ADM2_EN, ADM2_PCODE,\n           ADM1_EN, ADM1_PCODE,\n           status)) \n\n\ngwr_sf.fixed <- cbind(Osun_wp_sf_selected, gwr.fixed)\n\n\nVisualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\nprob_T <- tm_shape(Osun) +\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf.fixed) +  \n  tm_dots(col = \"yhat\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(8,14))\nprob_T\n\n\n\n\n\n\ntertiary_TV <- tm_shape(Osun)+ tm_polygons(alpha = 0.1) + tm_shape(gwr_sf.fixed) +\ntm_dots(col = “distance_to_tertiary_road_TV”, border.col = “gray60”, border.lwd = 1) + tm_view(set.zoom.limits = c(8,14))\ntmap_arrange(tertiary_SE, tertiary_TV, asp=1, ncol=2, sync = TRUE) ```"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#building-a-logistic-regression-models",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#building-a-logistic-regression-models",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "Building a Logistic Regression Models",
    "text": "Building a Logistic Regression Models\nIn the code chunk below, glm() of R is used to calibrate a logistic regression for the water point status.\n\nmodel <- glm(status ~  distance_to_primary_road + \n               distance_to_secondary_road + \n               distance_to_tertiary_road + \n               distance_to_city + \n               distance_to_town + \n               is_urban + \n               usage_capacity +\n               water_source_clean + \n               water_point_population + \n               local_population_1km, \n             data = Osun_wp_sf_clean, \n             family = binomial(link = 'logit'))\n\nInstead of using typical R report, blr_regress() of blorr package is used.\n\nblr_regress(model)\n\n                             Model Overview                              \n------------------------------------------------------------------------\nData Set    Resp Var    Obs.    Df. Model    Df. Residual    Convergence \n------------------------------------------------------------------------\n  data       status     4756      4755           4744           TRUE     \n------------------------------------------------------------------------\n\n                    Response Summary                     \n--------------------------------------------------------\nOutcome        Frequency        Outcome        Frequency \n--------------------------------------------------------\n   0             2114              1             2642    \n--------------------------------------------------------\n\n                                 Maximum Likelihood Estimates                                   \n-----------------------------------------------------------------------------------------------\n               Parameter                    DF    Estimate    Std. Error    z value     Pr(>|z|) \n-----------------------------------------------------------------------------------------------\n              (Intercept)                   1      0.3887        0.1124      3.4588       5e-04 \n        distance_to_primary_road            1      0.0000        0.0000     -0.7153      0.4744 \n       distance_to_secondary_road           1      0.0000        0.0000     -0.5530      0.5802 \n       distance_to_tertiary_road            1      1e-04         0.0000      4.6708      0.0000 \n            distance_to_city                1      0.0000        0.0000     -4.7574      0.0000 \n            distance_to_town                1      0.0000        0.0000     -4.9170      0.0000 \n              is_urbanTRUE                  1     -0.2971        0.0819     -3.6294       3e-04 \n           usage_capacity1000               1     -0.6230        0.0697     -8.9366      0.0000 \nwater_source_cleanProtected Shallow Well    1      0.5040        0.0857      5.8783      0.0000 \n   water_source_cleanProtected Spring       1      1.2882        0.4388      2.9359      0.0033 \n         water_point_population             1      -5e-04        0.0000    -11.3686      0.0000 \n          local_population_1km              1      3e-04         0.0000     19.2953      0.0000 \n-----------------------------------------------------------------------------------------------\n\n Association of Predicted Probabilities and Observed Responses  \n---------------------------------------------------------------\n% Concordant          0.7347          Somers' D        0.4693   \n% Discordant          0.2653          Gamma            0.4693   \n% Tied                0.0000          Tau-a            0.2318   \nPairs                5585188          c                0.7347   \n---------------------------------------------------------------\n\n\nIn the code chunk below, blr_confusion_matrix() of blorr package is used to compute the confusion matrix of the estimated outcomes by using 0.5 as the cutoff value.\n\nblr_confusion_matrix(model, cutoff = 0.5)\n\nConfusion Matrix and Statistics \n\n          Reference\nPrediction FALSE TRUE\n         0  1301  738\n         1   813 1904\n\n                Accuracy : 0.6739 \n     No Information Rate : 0.4445 \n\n                   Kappa : 0.3373 \n\nMcNemars's Test P-Value  : 0.0602 \n\n             Sensitivity : 0.7207 \n             Specificity : 0.6154 \n          Pos Pred Value : 0.7008 \n          Neg Pred Value : 0.6381 \n              Prevalence : 0.5555 \n          Detection Rate : 0.4003 \n    Detection Prevalence : 0.5713 \n       Balanced Accuracy : 0.6680 \n               Precision : 0.7008 \n                  Recall : 0.7207 \n\n        'Positive' Class : 1\n\n\nThe validity of a cut-off is measured using sensitivity, specificity and accuracy.\n\nSensitivity: The % of correctly classified events out of all events = TP / (TP + FN)\nSpecificity: The % of correctly classified non-events out of all non-events = TN / (TN + FP)\nAccuracy: The % of correctly classified observation over all observations = (TP + TN) / (TP + FP + TN + FN)\nTrue Positive (TP) : Events correctly classified as events.\nTrue Negative (TN) : Non-Events correctly classified as non-events.\nFalse Positive (FP): Non-events miss-classified as events.\nFalse Negative (FN): Events miss-classified as non-events.\n\nFor a standard logistic model, the higher is the cut-off, the lower will be the sensitivity and the higher would be the specificity. As the cut-off is decreased, sensitivity will go up, as then more events would be captured. Also, specificity will go down, as more non-events would miss-classified as events. Hence a trade-off is done based on the requirements. For example, if we are looking to capture as many events as possible, and we can afford to have miss-classified non-events, then a low cut-off is taken."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#building-geographically-weighted-logistic-regression-gwlr-models",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#building-geographically-weighted-logistic-regression-gwlr-models",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "Building Geographically Weighted Logistic Regression (gwLR) Models",
    "text": "Building Geographically Weighted Logistic Regression (gwLR) Models\n\nConverting from sf to sp data frame\nIn order to calibrate a geographically weighted logistic regression model by using GWModel package, we need to convert the sf data frame to sp object. This is because GWModel package only work with sp version of data frame.\nIn the code chunk below, as.Spatial() of sf package is used to convert Osun_wp_sf_clean from sf data frame to SpatialPolygonDataFrame. Also note that select() of dplyr package is used to select the variables of interest.\n\nOsun_wp_sp <- Osun_wp_sf_clean %>% \n  select(c(status, \n           distance_to_primary_road,\n           distance_to_secondary_road,\n           distance_to_tertiary_road,\n           distance_to_city, \n           distance_to_town, \n           water_point_population,\n           local_population_1km,\n           is_urban,\n           usage_capacity,\n           water_source_clean)) %>% \n  as_Spatial()\nOsun_wp_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 4756 \nextent      : 182502.4, 290751, 340054.1, 450905.3  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 11\nnames       : status, distance_to_primary_road, distance_to_secondary_road, distance_to_tertiary_road, distance_to_city, distance_to_town, water_point_population, local_population_1km, is_urban, usage_capacity, water_source_clean \nmin values  :      0,        0.014461356813335,          0.152195902540837,         0.017815121653488, 53.0461399623541, 30.0019777713073,                      0,                    0,        0,           1000,           Borehole \nmax values  :      1,         26909.8616132094,           19559.4793799085,          10966.2705628969,  47934.343603562, 44020.6393368124,                  29697,                36118,        1,            300,   Protected Spring \n\n\n\n\nBuilding Fixed Bandwidth GWR Model\n\nComputing fixed bandwidth\nBefore we can go ahead to calibrate the gwLR model, we need to determine the bandwidth to use. In the code chunk below, bw.ggwr() of GWModel is used to compute the optimal fixed distance bandwidth.\n\nbw.fixed <- bw.ggwr(status ~  distance_to_primary_road +\n                      distance_to_secondary_road + \n                      distance_to_tertiary_road + \n                      distance_to_city + \n                      distance_to_town + \n                      water_point_population + \n                      local_population_1km +\n                      is_urban +\n                      usage_capacity +\n                      water_source_clean, \n                    data=Osun_wp_sp, \n                    family = \"binomial\", \n                    approach =\"AIC\", \n                    kernel =\"gaussian\", \n                    adaptive = FALSE, \n                    longlat =FALSE)\n\n\nbw.fixed\n\n\ngwlr.fixed <- ggwr.basic(status ~  distance_to_primary_road +\n                           distance_to_secondary_road +\n                           distance_to_tertiary_road + \n                           distance_to_city + \n                           distance_to_town + \n                           water_point_population + \n                           local_population_1km +\n                           is_urban + \n                           usage_capacity + \n                           water_source_clean,\n                         data=Osun_wp_sp, \n                         bw = 2597.255, \n                         family = \"binomial\", \n                         kernel =\"gaussian\", \n                         adaptive = FALSE, \n                         longlat =FALSE)\n\n Iteration    Log-Likelihood\n=========================\n       0        -1957 \n       1        -1675 \n       2        -1525 \n       3        -1441 \n       4        -1403 \n       5        -1403 \n\n\n\ngwlr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-16 23:04:30 \n   Call:\n   ggwr.basic(formula = status ~ distance_to_primary_road + distance_to_secondary_road + \n    distance_to_tertiary_road + distance_to_city + distance_to_town + \n    water_point_population + local_population_1km + is_urban + \n    usage_capacity + water_source_clean, data = Osun_wp_sp, bw = 2597.255, \n    family = \"binomial\", kernel = \"gaussian\", adaptive = FALSE, \n    longlat = FALSE)\n\n   Dependent (y) variable:  status\n   Independent variables:  distance_to_primary_road distance_to_secondary_road distance_to_tertiary_road distance_to_city distance_to_town water_point_population local_population_1km is_urban usage_capacity water_source_clean\n   Number of data points: 4756\n   Used family: binomial\n   ***********************************************************************\n   *              Results of Generalized linear Regression               *\n   ***********************************************************************\n\nCall:\nNULL\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-124.555    -1.755     1.072     1.742    34.333  \n\nCoefficients:\n                                           Estimate Std. Error z value Pr(>|z|)\nIntercept                                 3.887e-01  1.124e-01   3.459 0.000543\ndistance_to_primary_road                 -4.642e-06  6.490e-06  -0.715 0.474422\ndistance_to_secondary_road               -5.143e-06  9.299e-06  -0.553 0.580230\ndistance_to_tertiary_road                 9.683e-05  2.073e-05   4.671 3.00e-06\ndistance_to_city                         -1.686e-05  3.544e-06  -4.757 1.96e-06\ndistance_to_town                         -1.480e-05  3.009e-06  -4.917 8.79e-07\nwater_point_population                   -5.097e-04  4.484e-05 -11.369  < 2e-16\nlocal_population_1km                      3.451e-04  1.788e-05  19.295  < 2e-16\nis_urbanTRUE                             -2.971e-01  8.185e-02  -3.629 0.000284\nusage_capacity1000                       -6.230e-01  6.972e-02  -8.937  < 2e-16\nwater_source_cleanProtected Shallow Well  5.040e-01  8.574e-02   5.878 4.14e-09\nwater_source_cleanProtected Spring        1.288e+00  4.388e-01   2.936 0.003325\n                                            \nIntercept                                ***\ndistance_to_primary_road                    \ndistance_to_secondary_road                  \ndistance_to_tertiary_road                ***\ndistance_to_city                         ***\ndistance_to_town                         ***\nwater_point_population                   ***\nlocal_population_1km                     ***\nis_urbanTRUE                             ***\nusage_capacity1000                       ***\nwater_source_cleanProtected Shallow Well ***\nwater_source_cleanProtected Spring       ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6534.5  on 4755  degrees of freedom\nResidual deviance: 5688.0  on 4744  degrees of freedom\nAIC: 5712\n\nNumber of Fisher Scoring iterations: 5\n\n\n AICc:  5712.099\n Pseudo R-square value:  0.1295351\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 2597.255 \n   Regression points: the same locations as observations are used.\n   Distance metric: A distance matrix is specified for this model calibration.\n\n   ************Summary of Generalized GWR coefficient estimates:**********\n                                                   Min.     1st Qu.      Median\n   Intercept                                -8.9630e+02 -4.9805e+00  1.7599e+00\n   distance_to_primary_road                 -1.9477e-02 -4.8092e-04  3.0174e-05\n   distance_to_secondary_road               -1.5757e-02 -3.7583e-04  1.2438e-04\n   distance_to_tertiary_road                -1.5673e-02 -4.2538e-04  7.6217e-05\n   distance_to_city                         -1.8447e-02 -5.6287e-04 -1.2745e-04\n   distance_to_town                         -2.2450e-02 -5.7335e-04 -1.5218e-04\n   water_point_population                   -5.2830e-02 -2.2810e-03 -9.8829e-04\n   local_population_1km                     -1.2757e-01  5.0016e-04  1.0647e-03\n   is_urbanTRUE                             -1.9866e+02 -4.3054e+00 -1.6908e+00\n   usage_capacity1000                       -2.0846e+01 -9.7311e-01 -4.1596e-01\n   water_source_cleanProtected.Shallow.Well -2.0782e+01 -4.5536e-01  5.3278e-01\n   water_source_cleanProtected.Spring       -5.2495e+02 -5.5983e+00  2.5500e+00\n                                                3rd Qu.      Max.\n   Intercept                                 1.2829e+01 1075.4234\n   distance_to_primary_road                  4.8497e-04    0.0143\n   distance_to_secondary_road                6.0665e-04    0.0259\n   distance_to_tertiary_road                 6.7104e-04    0.0129\n   distance_to_city                          2.3763e-04    0.0155\n   distance_to_town                          1.9318e-04    0.0225\n   water_point_population                    5.0564e-04    0.1313\n   local_population_1km                      1.8177e-03    0.0392\n   is_urbanTRUE                              1.2864e+00  746.9498\n   usage_capacity1000                        3.0334e-01    5.9492\n   water_source_cleanProtected.Shallow.Well  1.7870e+00   67.5549\n   water_source_cleanProtected.Spring        6.7736e+00  331.1243\n   ************************Diagnostic information*************************\n   Number of data points: 4756 \n   GW Deviance: 2792.323 \n   AIC : 4413.603 \n   AICc : 4747.217 \n   Pseudo R-square value:  0.5726785 \n\n   ***********************************************************************\n   Program stops at: 2022-12-16 23:05:17 \n\n\n\n\n\nModel Assessment\n\nConverting SDF into sf data.frame\nTo assess the performance of the gwLR, firstly, we will convert the SDF object in as data frame by using the code chunk below.\n\ngwr.fixed <- as.data.frame(gwlr.fixed$SDF)\n\nNext, we will label yhat values greater or equal to 0.5 into 1 and else 0. The result of the logic comparison operation will be saved into a field called most.\n\ngwr.fixed <- gwr.fixed %>%\n  mutate(most = ifelse(\n    gwr.fixed$yhat >= 0.5, T, F)) \n\n\ngwr.fixed$y <- as.factor(gwr.fixed$y)\ngwr.fixed$most <- as.factor(gwr.fixed$most)\nCM <- confusionMatrix(data=gwr.fixed$most, reference = gwr.fixed$y)\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE  1824  263\n     TRUE    290 2379\n                                          \n               Accuracy : 0.8837          \n                 95% CI : (0.8743, 0.8927)\n    No Information Rate : 0.5555          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.7642          \n                                          \n Mcnemar's Test P-Value : 0.2689          \n                                          \n            Sensitivity : 0.8628          \n            Specificity : 0.9005          \n         Pos Pred Value : 0.8740          \n         Neg Pred Value : 0.8913          \n             Prevalence : 0.4445          \n         Detection Rate : 0.3835          \n   Detection Prevalence : 0.4388          \n      Balanced Accuracy : 0.8816          \n                                          \n       'Positive' Class : FALSE           \n                                          \n\n\n\n\n\nVisualising gwLR\n\nOsun_wp_sf_selected <- Osun_wp_sf_clean %>% \n  select(c(ADM2_EN, ADM2_PCODE,\n           ADM1_EN, ADM1_PCODE,\n           status)) \n\n\ngwr_sf.fixed <- cbind(Osun_wp_sf_selected, gwr.fixed)\n\n\nVisualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\nprob_T <- tm_shape(Osun) +\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf.fixed) +  \n  tm_dots(col = \"yhat\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(8,14))\nprob_T\n\n\n\n\n\n\ntertiary_TV <- tm_shape(Osun)+ tm_polygons(alpha = 0.1) + tm_shape(gwr_sf.fixed) +\ntm_dots(col = “distance_to_tertiary_road_TV”, border.col = “gray60”, border.lwd = 1) + tm_view(set.zoom.limits = c(8,14))\ntmap_arrange(tertiary_SE, tertiary_TV, asp=1, ncol=2, sync = TRUE) ```"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#correlation-analysis",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#correlation-analysis",
    "title": "Modeling the Spatial Variation of the Explanatory Factors of Water Point Status in Osun State, Nigeria using Geographically Weighted Logistic Regression",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\n\nOsun_wp <- Osun_wp_sf_clean %>%\n  select(c(7,35:39,42:43, 46:47,57)) %>%\n  st_set_geometry(NULL) \n\n\ncluster_vars.cor = cor(\n  Osun_wp[,2:7])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#overview",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangling geospatial data using appropriate R packages."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "",
    "text": "In this in-class exercise, you are required to prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 downloaded from data.gov.sg.\n\nThe specific task of this in-class exercise are as follows:\n\nto import Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall in to RStudio environment,\nto import geospatial data in ESRI shapefile format into sf data frame format,\nto perform data wrangling by using appropriate functions from tidyverse and sf pakcges, and\nto visualise the distribution of passenger trip by using tmap methods and functions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Getting Started",
    "text": "Getting Started\nThree R packages will be used in this in-class exercise, they are:\n\ntidyverse for non-spatial data handling,\nsf for geospatial data handling,\ntmap for thematic mapping, and\nknitr for creating html table.\n\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, load these three R packages into RStudio.\n\n\n\npacman::p_load(tmap, sf, tidyverse, \n               knitr, h3jsr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#data-import",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#data-import",
    "title": "In-class Exercise 1",
    "section": "Data Import",
    "text": "Data Import\n\nImporting kml file\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\")\n\nReading layer `BusStop' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualising-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualising-geospatial-data",
    "title": "In-class Exercise 1",
    "section": "Visualising Geospatial Data",
    "text": "Visualising Geospatial Data\n\ntmap_mode(\"view\")\ntm_shape(busstop) +\n  tm_dots() +\n  tm_view(set.zoom.limits = c(11, 16))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#preparing-the-flow-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#preparing-the-flow-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Preparing the Flow Data",
    "text": "Preparing the Flow Data\n\nImporting the OD data\nFirstly, we will import the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\nA quick check of odbus tibble data frame shows that the values in OROGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. Hence, the code chunk below is used to convert these data values into character data type.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\nExtracting the study data\nFor the purpose of this exercise, we will extract commuting flows on weekday and between 7 and 9 o’clock.\n\norigtrip_7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\ndesttrip_7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nodbus7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nWe will save the output in rds format for future used.\n\nwrite_rds(odbus7_9, \"data/rds/odbus7_9.rds\")\n\nThe code chunk below will be used to import the save odbus7_9.rds into R environment.\n\nodbus7_9 &lt;- read_rds(\"data/rds/odbus7_9.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#working-with-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#working-with-geospatial-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Working with Geospatial Data",
    "text": "Working with Geospatial Data\nIn this section, you are required to import two shapefile into RStudio, they are:\n\nBusStop: This data provides the location of bus stop as at last quarter of 2022.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\n\nImporting geospatial data\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import BusStop downloaded from LTA DataMall into RStudio and save it as a sf data frame called busstop.\n\n\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n\nThe structure of busstop sf tibble data frame should look as below.\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import MPSZ-2019 downloaded from eLearn into RStudio and save it as a sf data frame called mpsz.\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\nThe structure of mpsz sf tibble data frame should look as below.\n\nglimpse(mpsz)\n\nRows: 332\nColumns: 7\n$ SUBZONE_N  &lt;chr&gt; \"MARINA EAST\", \"INSTITUTION HILL\", \"ROBERTSON QUAY\", \"JURON…\n$ SUBZONE_C  &lt;chr&gt; \"MESZ01\", \"RVSZ05\", \"SRSZ01\", \"WISZ01\", \"MUSZ02\", \"MPSZ05\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA EAST\", \"RIVER VALLEY\", \"SINGAPORE RIVER\", \"WESTERN …\n$ PLN_AREA_C &lt;chr&gt; \"ME\", \"RV\", \"SR\", \"WI\", \"MU\", \"MP\", \"WI\", \"WI\", \"SI\", \"SI\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"WEST…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"WR\", \"CR\", \"CR\", \"WR\", \"WR\", \"CR\", \"CR\",…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((33222.98 29..., MULTIPOLYGON (…\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() function of sf package is used to import the shapefile into R as sf data frame.\nst_transform() function of sf package is used to transform the projection to crs 3414."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#overview",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#overview",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "",
    "text": "Lesson 1 shares with you the basic principles of geospatial data models and geovisualisation. Upon completion of Hands-on 1, you have gained hands-on experiences on using R packages to import, wrangle, integrate and visualise geospatial data. In this in-class exercise, you will apply these newly acquired concepts, principles and methods in a real world case study.\n\n\nThe specific task of this in-class exercise are as follows:\n\nto import Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall in to RStudio environment,\nto import geospatial data in ESRI shapefile format into sf data frame format,\nto perform data wrangling by using appropriate functions from tidyverse and sf pakcges, and\nto visualise the distribution of passenger trip by using tmap methods and functions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Importing the OD data",
    "text": "Importing the OD data\nFirstly, we will import the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import origin_destination_bus_202308.csv downloaded from LTA DataMall into RStudio and save it as a tibble data frame called odbus.\n\n\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\n\n\n\nA quick check of odbus tibble data frame shows that the values in OROGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type.\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"4406…\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"1722…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\nThe taskThe solution\n\n\nUsing appropriate tidyverse functions to convert these data values into factor data type.\n\n\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\n\nNotice that both of them are in factor data type now.\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 44069, 20281, 2…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 17229, 20141, 2…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\nExtracting the study data\n\nThe taskThe solutin\n\n\nFor the purpose of this exercise, we will extract commuting flows on weekday and between 7 and 9 o’clock time intervals. Call the output tibble data table as origin7_9.\n\n\n\norigin7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\n\n\nIt should look similar to the data table below.\n\nkable(head(origin7_9))\n\n\n\n\nORIGIN_PT_CODE\nTRIPS\n\n\n\n\n01012\n1617\n\n\n01013\n813\n\n\n01019\n1620\n\n\n01029\n2383\n\n\n01039\n2727\n\n\n01059\n1415\n\n\n\n\n\nWe will save the output in rds format for future used.\n\nwrite_rds(origin7_9, \"data/rds/origin7_9.rds\")\n\nThe code chunk below will be used to import the save origin7_9.rds into R environment.\n\norigin7_9 &lt;- read_rds(\"data/rds/origin7_9.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Geospatial data wrangling",
    "text": "Geospatial data wrangling\n\nCombining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame.\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore bpundary.\n\n\n\nBefore moving to the next step, it is wise to save the output into rds format.\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.csv\")  \n\n\nThe taskThe solution\n\n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus7_9 data frame.\n\n\n\norigin_data &lt;- left_join(origin7_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C)\n\n\n\n\nBefore continue, it is a good practice for us to check for duplicating records.\n\nduplicate &lt;- origin_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nIf duplicated records are found, the code chunk below will be used to retain the unique records.\n\norigin_data &lt;- unique(origin_data)\n\nIt will be a good practice to confirm if the duplicating records issue has been addressed fully.\n\nThe taskThe solution\n\n\nNext, write a code chunk to update od_data data frame with the planning subzone codes.\n\n\n\nmpsz_origtrip &lt;- left_join(mpsz, \n                           origin_data,\n                           by = c(\"SUBZONE_C\" = \"ORIGIN_SZ\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-task",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-task",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "",
    "text": "In this in-class exercise, you are required to prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 downloaded from data.gov.sg.\n\nThe specific task of this in-class exercise are as follows:\n\nto import Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall in to RStudio environment,\nto import geospatial data in ESRI shapefile format into sf data frame format,\nto perform data wrangling by using appropriate functions from tidyverse and sf pakcges, and\nto visualise the distribution of passenger trip by using tmap methods and functions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-task-4",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-task-4",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "The task",
    "text": "The task\nFor the purpose of this exercise, we will extract commuting flows on weekday and between 7 and 9 o’clock time intervals. Call the output tibble data table as origin7_9."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-solutin",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-solutin",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "The solutin",
    "text": "The solutin\n\norigin7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#choropleth-visualisation",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#choropleth-visualisation",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Choropleth Visualisation",
    "text": "Choropleth Visualisation\n\nThe taskThe solution\n\n\nUsing the steps you had learned, prepare a choropleth map showing the distribution of passenger trips at planning sub-zone level.\n\n\n\ntm_shape(mpsz_origtrip)+\n  tm_fill(\"TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips generated at planning sub-zone level\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from URA\\n and Passenger trips data from LTA\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#overview",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#overview",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#getting-started",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#getting-started",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\n\n\nShow the code\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#the-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#the-data",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nImporting geospatial data\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\nShow the code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImporting attribute table\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\nShow the code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\nCombining both data frame by using left join\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\nShow the code\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)\n\n\n\n\nPlotting a choropleth map\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\nThe choropleth should look similar to ther figure below.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#deriving-contiguity-spatial-weights",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#deriving-contiguity-spatial-weights",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "Deriving Contiguity Spatial Weights",
    "text": "Deriving Contiguity Spatial Weights\nBy and large, there are two types of spatial weights, they are contiguity wights and distance-based weights. In this section, you will learn how to derive contiguity spatial weights by using sfdep.\nTwo steps are required to derive a contiguity spatial weights, they are:\n\nidentifying contiguity neighbour list by st_contiguity() of sfdep package, and\nderiving the contiguity spatial weights by using st_weights() of sfdep package\n\nIn this section, we will learn how to derive the contiguity neighbour list and contiguity spatial weights separately. Then, we will learn how to combine both steps into a single process.\n\nIdentifying contiguity neighbours: Queen’s method\nIn the code chunk below st_contiguity() is used to derive a contiguity neighbour list by using Queen’s method.\n\nnb_queen &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, queen argument is TRUE. If you do not specify queen = FALSE, this function will return a list of first order neighbours by using the Queen criteria. Rooks method will be used to identify the first order neighbour if queen = FALSE is used.\n\n\nThe code chunk below is used to print the summary of the first lag neighbour list (i.e. nb) .\n\nsummary(nb_queen$nb)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan province. The most connected area unit has 11 neighbours. There are two are units with only one neighbour.\nTo view the content of the data table, you can either display the output data frame on RStudio data viewer or by printing out the first ten records by using the code chunk below.\n\nnb_queen\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb   NAME_2  ID_3    NAME_3   ENGTYPE_3\n1                 2, 3, 4, 57, 85  Changde 21098   Anxiang      County\n2               1, 57, 58, 78, 85  Changde 21100   Hanshou      County\n3                     1, 4, 5, 85  Changde 21101    Jinshi County City\n4                      1, 3, 5, 6  Changde 21102        Li      County\n5                     3, 4, 6, 85  Changde 21103     Linli      County\n6                4, 5, 69, 75, 85  Changde 21104    Shimen      County\n7                  67, 71, 74, 84 Changsha 21109   Liuyang County City\n8       9, 46, 47, 56, 78, 80, 86 Changsha 21110 Ningxiang      County\n9           8, 66, 68, 78, 84, 86 Changsha 21111 Wangcheng      County\n10 16, 17, 19, 20, 22, 70, 72, 73 Chenzhou 21112     Anren      County\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\nThe print shows that polygon 1 has five neighbours. They are polygons number 2, 3, 4, 57,and 85.\nYou can reveal the county name of the five neighbouring polygons of popygon No. 1 (i.e. Anxiang) by using the code chunk below.\n\nnb_queen$County[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\n\n\nIdentify contiguity neighbours: Rooks’ method\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity neighbour list using Rooks’ method.\n\n\n\n\nShow the code\nnb_rook &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         .before = 1)\n\n\n\n\nIdentifying higher order neighbors\nThere are times that we need to identify high order contiguity neighbours. To accomplish the task, st_nb_lag_cumul() should be used as shown in the code chunk below.\n\nnb2_queen &lt;-  hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\nNote that if the order is 2, the result contains both 1st and 2nd order neighbors as shown on the print below.\n\nnb2_queen\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                        nb2\n1                                     2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\n2                           1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\n3                                                 1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\n4                                                             1, 2, 3, 5, 6, 57, 69, 75, 85\n5                                                 1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\n6                                         1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\n7                                                     9, 19, 66, 67, 71, 73, 74, 76, 84, 86\n8  2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\n9               2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\n10               11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#deriving-contiguity-weights-queens-method",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#deriving-contiguity-weights-queens-method",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "Deriving contiguity weights: Queen’s method",
    "text": "Deriving contiguity weights: Queen’s method\nNow, you are ready to compute the contiguity weights by using st_weights() of sfdep package.\n\nDeriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\nDeriving contiguity weights: Rooks method\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity weights using Rooks method.\n\n\n\n\nShow the code\nwm_r &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         wt = st_weights(nb),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#distance-based-weights",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-Spatial_Weights.html#distance-based-weights",
    "title": "In-class Exercise 2: Spatial Weights - sfdep methods",
    "section": "Distance-based Weights",
    "text": "Distance-based Weights\nThere are three popularly used distance-based spatial weights, they are:\n\nfixed distance weights,\nadaptive distance weights, and\ninverse distance weights (IDW).\n\n\nDeriving fixed distance weights\nBefore we can derive the fixed distance weights, we need to determine the upper limit for distance band by using the steps below:\n\ngeo &lt;- sf::st_geometry(hunan_GDPPC)\nnb &lt;- st_knn(geo, longlat = TRUE)\ndists &lt;- unlist(st_nb_dists(geo, nb))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_nb_dists() of sfdep is used to calculate the nearest neighbour distance. The output is a list of distances for each observation’s neighbors list.\nunlist() of Base R is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived.\n\n\n\nNow, we will go ahead to derive summary statistics of the nearest neighbour distances vector (i.e. dists) by usign the coced chunk below.\n\nsummary(dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nThe summary statistics report above shows that the maximum nearest neighbour distance is 65.80km. By using a threshold value of 66km will ensure that each area will have at least one neighbour.\nNow we will go ahead to compute the fixed distance weights by using the code chunk below.\n\nwm_fd &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_dist_band(geometry,\n                           upper = 66),\n               wt = st_weights(nb),\n               .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_dists_band() of sfdep is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\n\n\n\n\nDo It Yourself\n\n\n\nUsing the steps you learned in previous section, examine the data frame of the fixed distance weights.\n\n\n\n\nDeriving adaptive distance weights\nIn this section, you will derive an adaptive spatial weights by using the code chunk below.\n\nwm_ad &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_knn(geometry,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_knn() of sfdep is used to identify neighbors based on k (i.e. k = 8 indicates the nearest eight neighbours). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\nCalculate inverse distance weights\nIn this section, you will derive an inverse distance weights by using the code chunk below.\n\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_contiguity() of sfdep is used to identify the neighbours by using contiguity criteria. The output is a list of neighbours (i.e. nb).\nst_inverse_distance() is then used to calculate inverse distance weights of neighbours on the nb list."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Chapter 9: Global Measures of Spatial Autocorrelation and Chapter 10: Local Measures of Spatial Autocorrelation. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#overview",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#overview",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Chapter 9: Global Measures of Spatial Autocorrelation and Chapter 10: Local Measures of Spatial Autocorrelation. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#getting-started",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#getting-started",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\n\n\n\nShow the code\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#the-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#the-data",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nImporting geospatial data\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\n\nShow the code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nImporting attribute table\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\n\nShow the code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nCombining both data frame by using left join\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)\n\n\n\n\n\nShow the code\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nPlotting a choropleth map\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\nThe choropleth should look similar to the figure below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#global-measures-of-spatial-association",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#global-measures-of-spatial-association",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Global Measures of Spatial Association",
    "text": "Global Measures of Spatial Association\n\nStep 1: Deriving contiguity weights: Queen’s method\n\n\n\n\n\n\nDo it Yourself!\n\n\n\nUsing the steps you learned in previous lesson, derive a Queen’s contiguity weights by using appropriate spdep and tidyverse functions.\n\n\n\n\nDeriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\nComputing Global Moran’ I\nIn the code chunk below, global_moran() function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.\n\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\n\nPerforming Global Moran’sI test\nIn general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using global_moran_test() as shown in the code chunk below.\n\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe default for alternative argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality.\n\n\n\n\n\nPerforming Global Moran’I permutation test\nIn practice, monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_moran_perm()\nIt is alway a good practice to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\nset.seed(1234)\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nThe report above show that the p-value is smaller than alpha value of 0.05. Hence, reject the null hypothesis that the spatial patterns spatial independent. Because the Moran’s I statistics is greater than 0. We can infer the spatial distribution shows sign of clustering.\n\n\n\n\n\n\nReminder\n\n\n\nThe numbers of simulation is alway equal to nsim + 1. This mean in nsim = 99. This mean 100 simulation will be performed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#computing-local-morans-i",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\nIn this section, you will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations p_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations p_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative= -p_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.\n\n\nVisualising local Moran’s I\nIn this code chunk below, tmap functions are used prepare a choropleth map by using value in the ii field.\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\nVisualising p-value of local Moran’s I\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor p-values, the appropriate classification should be 0.001, 0.01, 0.05 and not significant instead of using default classification scheme.\n\n\n\n\nVisuaising local Moran’s I and p-value\nFor effective comparison, it will be better for us to plot both maps next to each other as shown below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\nVisualising LISA map\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "Hot Spot and Cold Spot Area Analysis (HCSA)\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#computing-local-gi-statistics",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#computing-local-gi-statistics",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Computing local Gi* statistics",
    "text": "Computing local Gi* statistics\n\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 16 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 17\n   gi_star   e_gi    var_gi p_value   p_sim p_folded_sim skewness kurtosis nb   \n     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;nb&gt; \n 1  0.0416 0.0114   6.41e-6  0.0493 9.61e-1         0.7      0.35    0.875 &lt;int&gt;\n 2 -0.333  0.0106   3.84e-6 -0.0941 9.25e-1         1        0.5     0.661 &lt;int&gt;\n 3  0.281  0.0126   7.51e-6 -0.151  8.80e-1         0.9      0.45    0.640 &lt;int&gt;\n 4  0.411  0.0118   9.22e-6  0.264  7.92e-1         0.6      0.3     0.853 &lt;int&gt;\n 5  0.387  0.0115   9.56e-6  0.339  7.34e-1         0.62     0.31    1.07  &lt;int&gt;\n 6 -0.368  0.0118   5.91e-6 -0.583  5.60e-1         0.72     0.36    0.594 &lt;int&gt;\n 7  3.56   0.0151   7.31e-6  2.61   9.01e-3         0.06     0.03    1.09  &lt;int&gt;\n 8  2.52   0.0136   6.14e-6  1.49   1.35e-1         0.2      0.1     1.12  &lt;int&gt;\n 9  4.56   0.0144   5.84e-6  3.53   4.17e-4         0.04     0.02    1.23  &lt;int&gt;\n10  1.16   0.0104   3.70e-6  1.82   6.86e-2         0.12     0.06    0.416 &lt;int&gt;\n# ℹ 78 more rows\n# ℹ 8 more variables: wts &lt;list&gt;, NAME_2 &lt;chr&gt;, ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;,\n#   ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;, geometry &lt;POLYGON [°]&gt;\n\n\n\n\nVisualising Gi*\n\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\nVisualising p-value of HCSA\n\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nVisuaising local HCSA\nFor effective comparison, you can plot both maps next to each other as shown below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#visualising-hot-spot-and-cold-spot-areas",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2-GLSA.html#visualising-hot-spot-and-cold-spot-areas",
    "title": "In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Visualising hot spot and cold spot areas",
    "text": "Visualising hot spot and cold spot areas\nNow, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\nFigure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "",
    "text": "Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#installing-and-loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#installing-and-loading-the-r-packages",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Installing and Loading the R Packages",
    "text": "Installing and Loading the R Packages\nAs usual, p_load() of pacman package will be used to check if the necessary packages have been installed in R, if yes, load the packages on R environment.\nFive R packages are need for this in-class exercise, they are: sf, sfdep, tmap, and tidyverse.\n\n\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#importing-geospatial-data",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\nIn the code chunk below, st_read() of sf package is used to import Hunan shapefile into R.\n\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#importing-attribute-table",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#importing-attribute-table",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Importing attribute table",
    "text": "Importing attribute table\nIn the code chunk below, read_csv() of readr is used to import Hunan_GDPPC.csv into R.\n\n\nGDPPC &lt;- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#computing-gi",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#computing-gi",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nNext, we will compute the local Gi* statistics.\n\nDeriving the spatial weights\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weights.\n\n\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nactivate() of dplyr package is used to activate the geometry context\nmutate() of dplyr package is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\n\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\nhead(GDPPC_nb)\n\n# A tibble: 6 × 5\n   Year County  GDPPC nb        wt       \n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;   \n1  2005 Anxiang  8184 &lt;int [6]&gt; &lt;dbl [6]&gt;\n2  2005 Hanshou  6560 &lt;int [6]&gt; &lt;dbl [6]&gt;\n3  2005 Jinshi   9956 &lt;int [5]&gt; &lt;dbl [5]&gt;\n4  2005 Li       8394 &lt;int [5]&gt; &lt;dbl [5]&gt;\n5  2005 Linli    8850 &lt;int [5]&gt; &lt;dbl [5]&gt;\n6  2005 Shimen   9244 &lt;int [6]&gt; &lt;dbl [6]&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#computing-gi-1",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#computing-gi-1",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\n\ngi_stars &lt;- GDPPC_nb %&gt;% \n  group_by(Year) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#arrange-to-show-significant-emerging-hotcold-spots",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#arrange-to-show-significant-emerging-hotcold-spots",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Arrange to show significant emerging hot/cold spots",
    "text": "Arrange to show significant emerging hot/cold spots\n\n\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#performing-emerging-hotspot-analysis",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_EHSA.html#performing-emerging-hotspot-analysis",
    "title": "In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Performing Emerging Hotspot Analysis",
    "text": "Performing Emerging Hotspot Analysis\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st, \n  .var = \"GDPPC\", \n  k = 1, \n  nsim = 99\n)\n\n\n\nVisualising the distribution of EHSA classes\nIn the code chunk below, ggplot2 functions ised used to reveal the distribution of EHSA classes as a bar chart.\n\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\nFigure above shows that sporadic cold spots class has the high numbers of county.\n\n\nVisualising EHSA\nIn this section, you will learn how to visualise the geographic distribution EHSA classes. However, before we can do so, we need to join both hunan and ehsa together by using the code chunk below.\n\n\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\n\nNext, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.\n\n\nehsa_sig &lt;- hunan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/data/geospatial/hexagon.html",
    "href": "In-class_Ex/In-class_Ex1/data/geospatial/hexagon.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 0 0     false"
  }
]